
\chapter{Introduction}

\section{Motivation}

In 1855 Mathew Fontaine Maury, nicknamed 'Pathfinder of the Seas', published his impressive dissertation titled \textit{The Physical Geography of the sea}. The paper begun the revolution in a sea transport system which allowed to shorten time of long travels and to save a lot of resources. Before publishing the book, Maury collected and processed almost 1.2 million of single data. It was certainly very arduous work, because people had to make the calculation instead of computers. The data came mainly from journey log books written by captains of United States Navy. The 'Pathfinder of the Seas' was one of the first men who realized that there is a special value hidden in huge sets of data. It just needs to be explored (\cite{Data}).  

160 years after Maury's discoveries, we notice the meaning of 'big data' almost everywhere: starting from pure science passing through finance, medicine or industry ending up at social media. Computers are capable of predicting whether a patient, who has had heart attack, will have a second heart attack or identify factors which are responsible for prostate cancer. It is also possible to predict a price of stock in 6 months from now, on the basis of company performance and economic data etc.
It is worth emphasizing that such deep analyses are achieved through fast computing systems. Nowadays processing of 1.2 million records, which had been being analyzed by Maury and his collaborators for probably a few years, takes just a while. Collecting required information is not a problem as well. According to information from \textit{IBM} website\footnote{\bibentry{IBM}}, every day we create 2.5 quintillion bytes of data. The data comes from various sources: sensors used to gather climate information, posts to social media sites, digital pictures and videos, cell phones \textit{GPS} signal, and so on.

As we see, the only thing left to do is to make sense of collected data: to extract important patterns and trends, and understand 'what the data says'. In other words, we need to learn from data. The branch of science which studies the problem of 'learning from data' is called machine learning. This paper is devoted to algorithms which solve the multi-label classification task which is one of the fundamental problems of machine learning. The details connected with machine learning aspects, especially with the multi-label classification, are presented in the next section.


\section{Machine learning tasks}

Machine learning is a branch of computer science which focuses on creating an automatic system which can learn and improve itself from a gathered experience (data). There are three main types of machine learning tasks:

\begin{itemize}
\item supervised learning,
\item unsupervised learning,
\item reinforcement learning.
\end{itemize}

In the supervised learning the experience is represented by a set of exemplary input data with desired outputs. This set is provided by a 'teacher' and the main goal is to find rules which allow to predict an output from a given input.

The reinforcement learning is a more complicated issue. In this case we usually consider a dynamical environment with various states and an agent which can perform one of available actions, in order to achieve a goal. In contrast to the supervised learning, there is no direct information about correct input-output pairs. As a result, an environment must be explored by an agent in order to gain a knowledge. In other words instead of a 'teacher' there is a 'nitpicker' which awards an agent for making a particular action. 

The last type of machine learning tasks, the unsupervised learning, aims at finding hidden structures in unlabelled data. In this case there are no error and reward signals which could be used to evaluate a solution. In fact, this is the most difficult and also the most challenging aspect of machine learning. 

In this study, as it has been already mentioned, we consider multi-label classification which is one of the fundamental supervised learning problems. 

The general classification problem aims at matching a specific category (also called class) to a new observation, on the basis of a training set. A training set is a set of observations whose class, they belong to, is known. Let us notice that in the classification problem we can distinguish two separated problems: the binary classification and the multi-class classification. The difference between them is certainly connected with a number of classes. In the first case we have only two categories, whereas in the second one there are many of them. In other words, multi-class classification is an extension of binary classification for more than two categories. However, in both cases there is always one target (an only one class is assigned to an object). If we increase a number of targets instead of a number of classes in binary classification, then we get the multi-label classification problem. In this case a subset of labels is predicted for a specific instance. Let us notice that there is also a relation between multi-class classification and multi-label classification. We can treat each subset of labels as an independent category and then simply transform the latter problem into the former one. 

It is worth emphasizing that the dependencies between particular classification problems are used to design most of multi-label algorithms. 

\section{Scope of thesis}

This thesis is devoted to multi-label methods which are strongly based on linear algebra. In practice this means that the algorithms use linear transformations to project a training set to a new reduced linear space at first, and then train linear regression in the reduced space. In the next chapters we can find mathematical background of these approaches, details of implementation and results of experiments performed on various data sets. However, we should consider if such an approach really makes sense at first. First of all let us walk through the most known concepts which are usually used to solve multi-label problems.

The simplest methods are based on transformation to multi-class problem. If we obtain a problem in such a form, we can solve it by one of standard learning algorithms (\textit{Naive Bayes classifier}, \textit{k-NN}, etc.). Unfortunately, such an approach has a significant defect which usually makes it useless. In fact, a number of classes grows exponentially with a number of labels!

The most popular approach is the \textit{binary relevance method (BR)} which transforms any multi-label problem into one binary problem for each label (it is sometimes also called \textit{one-against-all}). In fact, this method trains $k$ classifiers $C_1, \cdots, C_{k}$ - each classifier $C_j$ predicts a corresponding label $l_j$.  The main problem in this approach is connected with ignoring hidden information in a space of labels. This hidden information involves the interdependencies between particular labels. On the other hand, BR is simple and has low computational complexity. Moreover, it can be effectively parallelized because we build classifier independently for each label. In fact, these are its main strengths (\cite{Chain}).

The algorithms presented in this dissertation extend BR approach by preliminary projecting a training set to a new space. Let us notice that we can transform feature and label spaces into spaces of uncorrelated variables and then train a linear regressor for each label. In fact, we take into consideration this hidden information which is ignored by classic BR. Linear transformations can be also used to compress a training set in a sensible way. In fact, we save memory and time at the expense of losing accuracy. 

BR is certainly the elastic approach, so using linear regression is not required. However, this algorithm is very fast if it is well implemented. Its high efficiency is connected with the linear algebra operations such as matrices multiplication or matrix inversion that can be effectively performed in a multi-threaded environment. As a result, even for huge training sets, a sensible time of building classifier might be achieved. Unfortunately, we lose accuracy because of assumption of linearity between features and labels. 

It is worth emphasizing that for the purposes of the experiments the algorithms were implemented in C++11 programming language. This decision was dictated by an efficiency profit of using low-level programming language and a full control over a compilation process. Moreover, additional software tools have been implemented in order to perform computational experiments, for example, a parser for importing data sets in popular data formats (i.e. \textit{ARFF}) and a tool for measuring the predictive performance of the algorithms. As a result, these all elements were collected together and became a part of the library which can be used for multi-label classification.
