
\chapter{Introduction}

\section{Motivation}

In 1855 Mathew Fontaine Maury, nicknamed 'Pathfinder of the Seas', published his impressive dissertation titled \textit{The Physical Geography of the sea}. The paper begun the revolution in a sea transport system which allowed to shorten time of long travels and to save a lot of resources. Before publishing the book, Maury collected and processed almost 1.2 milion of single data. It was certainly very arduous work - people had to make the calculation instead of computers. The data came mainly from journey log books written by captains of United States Navy. The 'Pathfinder of the Seas' was one of the first men who realised that there is a special value hidden in huge sets of data. It just needs to be explored (\cite{Data}).  

160 years after Maury's discoveries, we notice the meaning of 'big data' almost everywhere: starting from pure science passing through finance, medicine or industry ending up at social media. Computers are capable of predicting whetcher a patient, who has had heart attack, will have a second heart attack or identify factors which are responsible for prostate cancer. It is also possible to predict a price of stock in 6 months from now, on the basis of company performance and economic data etc.
It is worth emphesising that such deep analyses are achieved through fast computing systems. Nowdays processing of 1.2 milion records, which had been being analysed by Maury and his collaborators for probably a few years, takes just a while. Collecting required information is not a problem as well. According to \textit{IBM}, every day we create 2.5 quintillion bytes of data. The data comes from various sources: sensors used to gather climate information, posts to social media sites, digital pictures and videos, cell phones \textit{GPS} signal, and so on (\cite{IBM}).

As we see, the only thing left to do is to make sense of collected data: to extract important patterns and trends, and understand 'what the data says'. In other words, we need to learn from data. The branch of science which studies the 'learning from data' issue is called \textit{Machine Learning}. This paper is devoted to algorithms which solve the multi-label classification task which is one of the fundamental problems in \textit{Machine Learning}. The details conncted with \textit{Machine Learning} aspects, especially with the multi-label classification, are presented in the next section.


\section{Discussing \textit{Machine Learning} tasks}

\textit{Machine Learning} is a branch of computer science which focuses on creating an automatic system which can learn and improve itself from a gathered experience (data). There are three main types of \textit{Machine Learning} tasks:

\begin{itemize}
\item supervised learning,
\item unsupervised learning,
\item reinforcement learning.
\end{itemize}

In the supervised learning the experience is represented by a set of exemplary input data with desired outputs. This set is provided by a 'teacher' and the main goal is to find rules which allow to predict an output from a given input.

The reinforcement learning is a more complicated issue. In this case we usually consider a dynamical environment with various states and an agent which can perform one of available actions, in order to achieve a goal. In contrast to the supervised learning, there is no any information about correct input-output pairs. As a result, an environment must be explored by an agent in order to gain a knowledge. In other words instead of a 'teacher' there is a 'nitpicker' which awards an agent for making a particular action. 

The last type of \textit{Machine Learning} tasks - the unsupervised learning aims at finding hidden structures in unlabelled data. In this case there are no error and reward signals which could be used to evaluate a solution. In fact, this is the most difficult and also the most challenging aspect of \textit{Machine Learning}. 

In this study, as it has been already mentioned, we consider only to the multi-label classification which is one of the basic problems connected with the supervised learning. 

The general classification problem aims at matching a specific category (also called class) to a new observation, on the basis of a traning set. A training set is a set of observations for whom a class, they belong to, is known. Let us notice that in the classification problem we can distinguish two separated problems: the binary classification and the multi-class classification. The difference between them is certainly connected with a number of classes. In the first case we have only two categories, whereas in the second one there are many of them. In other words, the mutlticlass classification is the extension of the binary classification for more than two categories. However, in both the cases there is always one target - only one class is assigned to an object. If we increase a number of targets instead of a number of classes in the binary classication, then we get the multi-label classification problem. This time a sequence of labels is predicted for a specific instance. Let us notice that there is also the relation between the multiclass classification and the multi-label classification. We can treat each sequence of labels as an independent category and then simply tranform one problem into an other. 

It is worth emphasising that the dependencies between particular classification problems are used to design most of multi-label algorithms. 

\section{Scope of thesis}

This paper is fully devoted to the multi-label methods which base mainly on linear algebra axioms. In practise it means that the algorithms use linear transformations to transform a whole training set to new linear spaces at first, and then train a classifier using linear regression. In the next chapters we can find mathematic background of these approaches, details of implementation and results of quality experiments conducted with various data sets. However, we should consider if such the approach really makes sense at first. First of all let us walk through the most known concepts which are usually used to solve multi-label problems.

The simplest concept is based on transformation to multi-class problem. If we obtain a problem in such a form, we can solve it by one of standard \textit{ML} algorithms (\textit{Naive Bayes classifier}, \textit{k-NN}, etc.). Unfortunately, such the approach has a significant defect which usually makes it useless - a number of classes grows exponentially with a number of labels!

The most popular approach is the \textit{binary relevance method (BM)} which transforms any multi-label problem into one binary problem for each label (it is sometimes also called \textit{one-against-all}). In fact, this method trains $|L|$ classifiers $C_1, \cdots, C_{|L|}$ - each classifier $C_j$ predicts a corresponding label $l_j$. The main problem in this approach is connected with ignoring relations between particular labels. On the other hand, \text{BM} is simple and has low computational complexity - let us notice that it can be parallelized thanks to the assumption of independecy of labels. In fact, these are its main strengths (\cite{Chain}).

The method which allow to overcome the label independece assumption is \textit{The Classifier Chain Model (CC)}. Anagolous to \textit{BB}, \textit{CC} takes into account $|L|$ classifiers, however this time these classifiers are linked along a chain. The feature space of each link in the chain is extended with the $0/1$ label associations of all previous links. In terms of time complexity, \textit{CC} can be close to \textit{BM} (it depends on a number of labels). However, it requires more memory, because of extending a feature space for each next classifer in a chain. Moreover the chaining procedure implies that \textit{CC} cannot be parallelized (\cite{Chain}).

The algorithms presented in this dissertation might join adventages of \textit{BM} and \textit{CC} approaches. Let us notice that we can transform feature and label spaces to spaces of ucorrelated variables (\textit{PCA} is a good example of such the transfomation) and then train a linear regressor for each label. Linear transformations can be also used to compress a training set in a sensible way - we save memory and time at the expense of losing accuracy. It is also worth mentioning that the linear algebra operations such as matrices multiplication or matrix invertion can be effectively performed in a multithreaded environment what means that a training procedure can be computed efficiently. As a result, even for huge training sets, a sensible time of building classifier might be achieved. 

It is worth emphasising that for the purposes of the experiments the algorithms were implemented in \textit{C++11} programming language. This decision was dictated by an efficiency profit of using low-level programming language and a full control over a compilation process. The quality experiment required creating additional components, such as parsers for popular data formats (i.e. \textit{ARFF}) or tools which measure a quality of classification. As a result, these all elements were collected together and became a part of the library which can be installed in an operating system.
