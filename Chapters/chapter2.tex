
\chapter{Background}
\section{Notation}

We will denote random variables by uppercase letters $W$, $X$ etc. If $X$ is a vector (multidimensional random variable) then its components will be denoted by $X_i$. Specific observed values of $X$ are written by lowercase letters $x_i$ or $x_{ij}$ if $X$ is a vector. Matrices are denoted by bold uppercase $\boldsymbol{W}$, $\boldsymbol{X}$ in order to distinguish them from random variables. Similarly, vectors are represented by lowercase bold letters $\boldsymbol{w}$, $\boldsymbol{x}$ etc. For a matrix $\boldsymbol{X}$, $\boldsymbol{x}_{i}$ denotes its $i-th$ row. Moreover, let us assume that all vectors are column vectors. 

$\boldsymbol{X} \in \mathbb{R}^{m \times n}$ symbol means that a matrix $\boldsymbol{X}$ has $m$ rows and $n$ columns and its elements are real numbers. For a $c \times d$  matrix $\boldsymbol{X}$, $\norm{\textbf{X}}_{F}$ represents its Frobenius norm, $\textbf{X}^P$ is its pseudoinverse and $\textbf{A}^T$ is a matrix transposed to $\textbf{A}$. 
Let us also define a few useful functions:
\begin{itemize}
\item $tr(\boldsymbol{X})$ is a trace of a matrix $\boldsymbol{X}$, 
\item $randn(n,k)$ generates an $n \times k$ random matrix using the normal distribution,
\item $eig(\boldsymbol{X})$ returns eigenvalues and eigenvectors of a matrix $\boldsymbol{X}$,
\item $round(\boldsymbol{x})$ rounds all values of a vector $\boldsymbol{x}$ to $1$ or $0$.
\item $orthogonalize(\boldsymbol{X})$ returns orthonormal basis of a range space of a matrix $\boldsymbol{X}$,
\item $inv(\boldsymbol{X})$ inverses a matrix $\boldsymbol{X}$,
\item $head\_cols(\boldsymbol{X},h)$ returns first $h$ left columns of a matrix $\boldsymbol{X}$,
\item $push\_front\_column(c,\boldsymbol{X})$ inserts a column $\boldsymbol{c}$ into $\boldsymbol{X}$ as a first column,
\item $push\_back\_column(c,\boldsymbol{X})$ push a column $\boldsymbol{c}$ at the end of $\boldsymbol{X}$,
\item $rank(\boldsymbol{X})$ is a column rank of a matrix $\boldsymbol{X}$,
\item $corr(x,y)$ return a correlation between $x$ and $y$ . 

\end{itemize}

\section{Formal multi-label classification task definition}

Given the value of an input vector (vector of features) $X$, we want to make a good prediction of an output vector (vector of labels) $Y$, which is usually denoted by $\hat{Y}$. Looking at this more formally, we look for a function $f$, such that:   

\begin{equation}\label{eq:def}
    f: X \rightarrow \{0,1\}^k 
\end{equation}

To find such the function, we certainly need data, usually a lot of it. Therefore, we assume we have available a set of measurements $\{(X_1, Y_1), (X_2, Y_2), \dots (X_m, Y_m)\}$, $Y_i\in\{0,1\}^k$, known as the training set.  

\section{Solution based on linear regression model}

In this section we focus on a solution based on linear regression (\textit{LR}). As it has been already mentioned, this method is our basic concept which is used by all algorithms presented in this paper. \textit{LR} is a good example of the \textit{binary relevance} approach - a regressor is computed independently for all labels. Let us now go through background which is behind linear regression. 

Given a data set of observations $\{y_i, x_{i1}, x_{i2}, \dots, x_{in}\}_{i=1}^{m}$, the linear regression analysis aims at studying a relationship between a dependent variable $Y$ and one or more explanatory variables $X_{j}$. The model of this relationship is described by a hyperplane which is given by the following formula:

\begin{equation}\label{eq:LR1}
    \hat{Y} = \beta_0X_{0} + \beta_1X_{1} + \beta_2X_{2} + \dots + \beta_nX_{n}
\end{equation}
where $\hat{Y}$ is a predictor of a variable $Y$. 
There is certainly an infinitive number of hyperplanes which can model a relationship for a given data set. However, we want the prediction error (difference between predicted value and real value) to be as low as it is possible. Furthermore, let us notice that adding $X_j$ variable to the model is used to explain the part of $Y$ that has not been explained by $X_{j-1}, X_{j-2}, \cdots X_1$. It means that the more explanatory variables we have, the more accurate model we get (\cite{Weisberg}).

There are a few approaches which can be used to find the best fitted hyperplane. We focus on method which is called the method of \textit{least squares}. In this method we look for coefficents $\beta_i$ that minimize the residual sum of squares. In general it is comfortable to express all calculations in matrix notation. Let $\boldsymbol{X} \in \mathbb{R}^{m \times (n+1)}$ be a design matrix (a matrix of values of independent variables) with each row an input vector (with a $1$ in the first position), $\boldsymbol{y} \in \mathbb{R}^{m}$ a vector of values of a dependent variable, $\boldsymbol{\beta} \in \mathbb{R}^{n+1}$ a vector of coefficents of the model and $\epsilon \in \mathbb{R}^{m}$ a vector of prediction errors. The model is then described by the formula:

\begin{equation}\label{eq:LR1}
    \boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon} 
\end{equation}
In order to estimate a vector $\boldsymbol{\beta}$, we need to solve the following optimization problem:

\begin{equation}\label{eq:LR2}
    \boldsymbol{\hat{\beta}} = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}}\sum\limits_{i=1}^{m}(\boldsymbol{y}_{i}-\sum\limits_{j=1}^{n+1}\boldsymbol{x}_j\boldsymbol{\beta}_j)^2
\end{equation}
which in matrix notation is expressed as:

\begin{equation}\label{eq:LR3}
    \boldsymbol{\hat{\beta}} = (\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})
\end{equation}

The residual sum of squares is certainly a quadratic function in the $n+1$ parameters, so we can diffrentiate it with respect to $\boldsymbol{\beta}$:

\begin{equation}\label{eq:LR4}
    \nabla_{\boldsymbol{\beta}} (\boldsymbol{y}-\boldsymbol{X}\boldsymbol{w})^T(\boldsymbol{y}-\boldsymbol{X}\bold{w}) = -2\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})
\end{equation}
The last thing which left to do is to set the first derivative to zero (we assume that $\boldsymbol{X}$ has full rank):

\begin{equation}\label{eq:LR4}
    \boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}) = 0
\end{equation}
in order to obtain a unique solution:

\begin{equation}\label{eq:LR5}
    \boldsymbol{\hat{\beta}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
The calculation of the derivetive in \Cref{eq:LR4} is a bit tricky - it uses a trace of matrix and its properties. The full proof of \Cref{eq:LR5} can be found in \Cref{app:least}. Let us notice that although a matrix $\boldsymbol{X}^T\boldsymbol{X}$ is squared, there is no certainty that it is invertable. Thus, it is safely to use a pseudoinverse (\cite{Trevor}).

If we treat a matrix $\boldsymbol{X}$ as a feature space and a vector $\boldsymbol{y}$ as a vector of a label space, we can simply use it in multi-label tasks. \Cref{alg:LR0} presentes the training procedure.

\begin{algorithm}
    \caption{Linear regression based classifier (I)}\label{alg:LR0}
    \begin{algorithmic}[1]
    \Procedure{build\_lr\_classifier}{$\boldsymbol{X}$,$\boldsymbol{Y}$}
        \State $\boldsymbol{Z} \gets \boldsymbol{X}$ 
        \State $\boldsymbol{O} \gets [1, 1, \cdots]^T$ 
        \State $\boldsymbol{\beta} \gets []$
        \State $push\_front\_column(\boldsymbol{O}, \boldsymbol{Z})$
        \For {each column $\boldsymbol{y}$ of $\boldsymbol{Y}$}
            \State $push\_back\_column($ $(\boldsymbol{Z^T}\boldsymbol{Z})^P\boldsymbol{Z^T}\boldsymbol{y}$, $\boldsymbol{\beta})$ 
        \EndFor
        \State \textbf{return} $\boldsymbol{\beta}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}
There is certainly nothing suprising in \Cref{alg:LR0} - we simply create the model independently for each label. However, let us notice that the result of $(\boldsymbol{Z^T}\boldsymbol{Z})^P\boldsymbol{X^T}$ does not depend on $\boldsymbol{y}$. We can use this fact and calculate it ones instead of doig that in each iteration. The second version of the algorithm, which contains such the modification, is shown in \Cref{alg:LR1}. 

\begin{algorithm}
    \caption{Linear regression based classifier (II)}\label{alg:LR1}
    \begin{algorithmic}[1]
    \Procedure{build\_lr\_classifier}{$\boldsymbol{X}$,$\boldsymbol{Y}$}
        \State $\boldsymbol{Z} \gets \boldsymbol{X}$ 
        \State $\boldsymbol{O} \gets [1, 1, \cdots]^T$ 
        \State $\boldsymbol{\beta} \gets []$
        \State $push\_front\_column(\boldsymbol{O}, \boldsymbol{Z})$
        \State $\boldsymbol{H} \gets (\boldsymbol{Z^T}\boldsymbol{Z})^P\boldsymbol{Z^T}$
        \For {each column $\boldsymbol{y}$ of $\boldsymbol{Y}$}
            \State $push\_back\_column($ $\boldsymbol{H}\boldsymbol{y}$, $\boldsymbol{\beta})$ 
        \EndFor
        \State \textbf{return} $\boldsymbol{\beta}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}


Having a trained classifier, we need a procedure which predicts a set of labels. This one is also simple and is presented in \Cref{alg:CL1}

\begin{algorithm}
    \caption{Prediction algorithm for \textit{LR} classifier}\label{alg:CL1}
    \begin{algorithmic}[1]
    \Procedure{build\_lr\_classifier}{$\boldsymbol{x}$, $\boldsymbol{\beta}$}
        \State $\boldsymbol{z} \gets \boldsymbol{x}$ 
        \State $push\_front\_column([1], \boldsymbol{z}^T)$ 
        \State \textbf{return} $round(\boldsymbol{z}^T\boldsymbol{\beta})$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

There are, of course, prons and cons of the approach based on \textit{LR}. First of all, the method is fast, simple and rather easy to implement - these are its main strengths. On the other hand, relations between labels are not taken into account (\textit{one-against-all}). Moreover, the final model is linear, so the algorithm makes the following assumptions:

\begin{itemize}
    \item linearity between features and particular labels,
    \item error coefficents in the error matrix are uncorrelated with each other - they are characterised by the normal distribution,
    \item a number of examples should be higher than a number of features in a design matrix,
\end{itemize}

Furthermore, we are not often satisfied with the least squares estimates. The problem is with prediction accuracy, because the estimates often have low bias but large variance. Such the situation is characteristic for overfitting. We can deal with that by sacrificing a little bit bias in order to reduce the variance. The easiest way, to achieve that, is to select a subset of variables (features) by some strategy and then create a model. However, because the selection is a discrete process - variables are retained or discarded - it still often exhibits high variance. Thus, let us use \textit{regularization} (\textit{shrinkage}) process which is more continuous and does not suffer as much from high variability (\cite{Trevor}).

In general, the \textit{regularization} refers to a process of adding additional information to a mathematical or a statistical model. The most commonly used method of \textit{regularization} is \textit{Tikhonov regularization}. This method deals well with problems which are solved by the \textit{least squared} estimation. If we want \Cref{eq:LR2} to be given the regularization, we need to include the additional term. This is presented by the following formula:

\begin{equation}\label{eq:LR6}
    \boldsymbol{\hat{\beta}} = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}}\{\sum\limits_{i=1}^{m}(\boldsymbol{y}_{i}-\sum\limits_{j=1}^{n+1}\boldsymbol{x}_j\boldsymbol{\beta}_j)^2+\lambda\sum\limits_{j=2}^{n+1}\boldsymbol{\beta}_j^2\}
\end{equation}
where $\lambda \geq 0$ is a complexy parameter which controls the amount of \textit{shrinkage}: the larger the value of $\lambda$, the greater the amount of \textit{shrinkage}.  The solution of \Cref{eq:LR6} taking into account \textit{Tikhonov regularization}, is then equivalent to:

\begin{equation}\label{eq:LR7}
    \boldsymbol{\hat{\beta}} = (\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
where $I$ is an $(n+1)\times(n+1)$ the identity matrix (\cite{Trevor}). 
Let us notice that this time there is no need to use pseudoinverse - $\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}$ is invertible. Let us prove this fact. In the proof we will use one of the most known matrix decomposition which is called \textit{Singular Value Decomposition}(\textit{SVD}). More details connected with \text{SVD} is in \Cref{app:svd} - it is worth looking at that, because this decomposition is used in the rest of presented algorithms. Using \textit{SVD} decomposition of a symmetric matrix $\boldsymbol{X}^T\boldsymbol{X}$ and its properties, we can express $\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}$ as: 
\begin{equation}\label{eq:inv_proof}
\begin{split}
    \boldsymbol{X}^T\boldsymbol{X} + \lambda\boldsymbol{I}&=\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^T + \lambda\boldsymbol{I} = \boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^T + \lambda\boldsymbol{V}\boldsymbol{V}^T\boldsymbol{I}\boldsymbol{V}\boldsymbol{V}^T \\ 
    = &\boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{V}^T\boldsymbol{I}\boldsymbol{V})\boldsymbol{V}^T = \boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{I})\boldsymbol{V}^T 
\end{split}
\end{equation}
Let us notice that $\boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{I})\boldsymbol{V}^T$ is invertible, becasue $\boldsymbol{V}$ is invertible (it is orthogonal) and $\boldsymbol{\Sigma} +\lambda\boldsymbol{I}$ is invertible as well (this is a diagonal matrix and we have certainty that it does not have zeros on its diagonal). The learning procedure of regularized \textit{LR} classifier is shown in \Cref{alg:LR2}

\begin{algorithm}
    \caption{Linear regression based classifier (III)}\label{alg:LR2}
    \begin{algorithmic}[1]
    \Procedure{build\_lr\_classifier}{$\boldsymbol{X}$,$\boldsymbol{Y}$}
        \State $\boldsymbol{Z} \gets \boldsymbol{X}$ 
        \State $\boldsymbol{O} \gets [1, 1, \cdots]^T$ 
        \State $\boldsymbol{\beta} \gets []$
        \State $push\_front\_column(\boldsymbol{O}, \boldsymbol{Z})$
        \State $\boldsymbol{H} \gets (\boldsymbol{Z^T}\boldsymbol{Z}+\lambda\boldsymbol{I})^P\boldsymbol{Z^T}$
        \For {each column $\boldsymbol{y}$ of $\boldsymbol{Y}$}
            \State $push\_back\_column($ $\boldsymbol{H}\boldsymbol{y}$, $\boldsymbol{\beta})$ 
        \EndFor
        \State \textbf{return} $\boldsymbol{\beta}$
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

Both the solutions of the linear regression problem: with the regularization and without it have similar complexity. The operations which affect the complexity in these approaches, are: multiplying and inverting matrices which can be computed in $O(n^{2.373})$ time by algorithms based on \textit{Coppersmith–Winograd} method (\cite{VVW}). 

\section{Feature and label space transformations with PCA}

In this chapter we will discuss multi-label task algorithms which transforms a feature or/and a label space by \textit{PCA} algorithm. As it has been already mentioned, there are two goals which we would like to achieve by such the transformations:
\begin{itemize}
    \item The reduction (compression) of a feature or/and label space. Let us notice that in \textit{LR} classifier, we have to invert a $\boldsymbol{X^T}\boldsymbol{X}$ matrix. If we discard redundunt variables, we can make this operation faster and decrease the complexity of the algorithm.
    \item Transformation to the space where labels are uncorrelated. \textit{LR} classifier does not take into account correlation between labels. Unfortunately, it has an influence on accuracy of prediction. Do such the transformation, we simply eliminate this problem. 
\end{itemize}

\subsection{Standard \textit{PCA} method}

The main concept, which is behind \textit{PCA}, is to reduce dimensionality of a data set consisting of a large number of iterrelated variables, while retaining as much as possible of the variation present in a data set. Such the reduction can be achieved by transforming to a new set of variables, the principal components, which are uncorrelated, and which are ordered so that the first few retain most of variation present in all of the original variables. Computation of the principal components reduces to the solution of an eigenvalue-eigenvector for a positive-semifdefinite symmetric matrix (a covariance or correlation matrix) \cite{Jolliffe}. Looking at this more intuitively, the data set can be considered as a cloud of points while variables can be thought as their coordinates. All what we have to do is to find new coordinate system (ie. by rotation) such that the greatest variance by some projection of the data comes to lie on the first coordinate, the second greatest variance on the second coordinate, and so on. More information about eigenvalues and eigenvectors can be found in \Cref{app:eigen}. 

Finding \textit{PCs} can be obtained by \textit{Singular Value Decomposition}. More information about \textit{PCA} and its connection with eigenvalues and eigenvectors can be found in \Cref{app:svd}. Let $X \in \mathbb{R}_{m \times n}$ be an input matrix where $n$ is a number of variables. Moreover let us assume that $X$ is centered (the mean in each column is equal to $0$). A covariance matrix $C \in \mathbb{R}_{n \times n}$ is then equal to $\frac{X^TX}{n-1}$. Taking into account that $C$ is a symmetric matrix, its \textit{SVD} decomposition is equivalent to:

\begin{equation}\label{eq:pca1}
    C=Q\Sigma{Q^T}
\end{equation}
where $V \in \mathbb{R}^{m \times m}$ is an orthogonal matrix of eigenvectors (called also principal axes), while $S \in \mathbb{R}^{m \times m}$ is a diagonal matrix with eigenvalues in the decreasing order on its diagonal. 
Let us now consider \textit{SVD} decomposition of $X$:

\begin{equation}\label{eq:pca2}
    X=USV^{T}
\end{equation}
It is easy to see that a covariance matrix $C$ might be expressed by \textit{SVD} decomposition of $X$:

\begin{equation}\label{eq:pca3}
    C=\frac{X^TX}{n-1} = \frac{(USV^T)^T(USV^T)}{n-1} = \frac{VSU^TUSV^T}{n-1} = V\frac{S^{2}}{n-1}V^{T} 
\end{equation}
In fact, there is no need to use a covariance matrix to compute its principal axes and its eigenvalues - \textit{SVD} of a matrix $X$ is sufficent \cite{Jolliffe}. In order to obtain a new space for data (prinicipal components), the following equation can be used:

\begin{equation}\label{eq:pca4}
    PCs = XV = (USV^{T})V = US
\end{equation}
The reduction of data can be made by leaving first $h$ columns of a matrix $PCs$ , where $h<n+1$.

Features and/or labels reduction by \textit{PCA} can be simply combined with a classifier based onthe linear regression model. At first data are transformed into a principal components space. A new space is then reduced and finally a regressor can be learnt from such encoded data. If a classifier is learnt from an encoded label space, then a score of prediction must be decoded to the original space. All required steps are presented in \Cref{alg:pca1}.

\begin{algorithm}
    \caption{Multi-dimension linear regressor with preliminary data reduction}\label{alg:pca1}
    \begin{algorithmic}[1]
    \Function{computePCA}{$X \in \mathbb{R}^{m \times n}$} 
        \State $\text{Let } Z=[z_1 \cdots z_m] \text{ with } z_i = x_i-\bar{x}$
        \State $\text{Perform \textit{SVD} on } Z \text{ to obtain } Z = U\Sigma{V^T}$
        \State $\text{return } (U,\Sigma,V)$
    \EndFunction
    \item[] 
        \State Let $\hat{X}$ be a copy of $X$ and $\hat{Y}$ be a copy of $Y$
    \If{reduceX == true}
        \State $(U_x, \Sigma_{x}, V_x) = computePCA(X,h1)$
        \State $\hat{X}=head\_cols(U_x\Sigma_{x}, h1)$
    \EndIf
    \If{reduceY == true}
        \State $(U_y, \Sigma_{y}, V_y) = computePCA(Y,h2)$
        \State $\hat{Y}=head\_cols(U_y\Sigma_{y}, h2)$
    \EndIf
    \State Learn linear regressor $r(\hat{X})$ from $\{\hat{X},\hat{Y}\}$
    \item[]
    \State Let $\hat{x}$ be a copy of instance $x$
    \If{reduceX == true}
    \State $\hat{x} = (\hat{x}^TV_x)^T$
    \EndIf
    \If{reduceY == true}
    \State Predict the label-set of $\hat{x}$ by $h(\hat{x})=round( r(\hat{x}) * head\_rows(V_y^T, h2) + \bar{y})$ 
    \Else
    \State Predict the label-set of $\hat{x}$ by $h(\hat{x})=round( r(\hat{x}) )$ 
    \EndIf

    \end{algorithmic}
\end{algorithm}

Creating a regressor from reduced data has certainly lower complexity than from original data, however \textit{PCA} algorithm costs a time. The implementation based on \textit{SVD} decomposition for an $m\times{n}$ matrix is $O(m^2n + n^3)$, so it is hard to expect this method works fast. It is also important to remember that \textit{PCA} might be understood as a lossy compression. For some data sets it might prevent a classifier from being overfitted, but it might also decrease its quality - it is naturally connected with a compression degree. 
One of possible ideas, which might deal with the complexity issue, is to use faster \textit{PCA} algorithm which is not so accurate and gives only an estimation of principal components. Such an algorithm is described in the next section.

\subsection{Random \textit{PCA} algorithm}

In order to make a computation of principal components faster, we can use a randomized version of the \textit{PCA} algorithm. The full method is presented in \Cref{alg:rpca1}. 

\begin{algorithm}
    \caption{Randomized PCA}\label{alg:rpca1}
    \begin{algorithmic}[1]
    \Function{RPCA}{$k$, $X \in \mathbb{R}^{m \times n}$} 
        \State $(p,q) \leftarrow \text{(20,1)}$
        \State $Q \leftarrow \text{randn(n, k+p)}$
            \For{$i \in \{1,\cdots,q\}$}
            \State $\psi \leftarrow X^TXQ$
            \State $Q \leftarrow \text{orthogonalize($\psi$)}$
            \EndFor
            \State $F \leftarrow (X^TXQ)^T(X^TXQ)$
            \State $(V,\Sigma^2) \leftarrow \text{eig($F$,$k$)}$
            \State $V \leftarrow (X^TXQ)V\Sigma^P$
            \State $\text{return } (V, \Sigma)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The algorithm takes two parameters on its input: data matrix $X \in \mathbb{R}^{m \times n}$ and a compression degree $k$ (a number of eigenvectors that are taken into account). The first step in this recipe is to find a range of a covariance matrix $X^TX$. The range of a matrix $A$ can be understood as a collection of vectors $b$ which satisfy the equation $Ax=b$. In other words, it is a set of all possible linear combinations of column vectors of a matrix $A$.  Let us notice that eigenvectors might appear frequently as solutions of this equation. In fact, a range of $\psi$ will tend to be more aligned with the space spanned by the top eigenvectors of $X^TX$ \cite{Mineiro}. 

The randomized range finder begins in 4. At first we start with a set of $k+p$ random vectors and probe a range. Next an orthogonal basis for $\psi$ is computed. This operation is repeated $q$ times. We can certainly manipulate a parameter of $q$, however it should be rather low, beacues the complexity of computing an orthogonal basis is $O(nk^2)$. 
After finding a good aproximation for a principal subspace of $X^TX$, we optimize fully over that subspace and back out the solution. It is worth emphasising that the last steps, including eigendecoposition of $F \in \mathbb{R}^{(k+p) \times (k+p)}$, are cheap. It is caused by low dimensions of a matrix $F$ \cite{Mineiro}.

The full classifying algorithm is very similar to \Cref{alg:pca1}. We just have to replace standard \textit{PCA} algorithm with the random one.

\section{Label space reduction inspired by \textit{CCA}}

Multi-label classifiers based on preliminary \textit{PCA} compression of data have an essential disadventage - a reduction of a label (feature) space is made independently of a feature (label) space. In other words, a label space is not aware of a reduction of feature space. One of possibilities, to deal with this issue, is to use a statistical method called \textit{Canonical Correlation Analysis} or rather methods inspired by \textit{CCA}.   

\subsection{Canonical correlation analysis}

Let us assume we are given two data sets (matrices): $X \in \mathbb{R}^{m \times n_1}$ and $Y \in \mathbb{R}^{m \times n_2}$. Without loss of generality we suppose that we have already substracted the mean in each column. \textit{CCA} aims at finding a linear combination of $X$ coordinates that correlates well over the data with a linear combination of $Y$ coordinates. In other words, we want to find the best matched pair of the linear combination of $X$ and $Y$ which have the largest coefficent of correlation. We certainly do not need to stop with a first-best pair - we can ask for a second-best pair, a third-best pair and so on \cite{William}.

Formally, this problem can be expressed by two equations:
\begin{equation}\label{eq:cca1}
    U=XW_x^T,   V=YW_y^T    
\end{equation}
where $W_x \in \mathbb{R}^{d \times n_1}$, $W_y \in \mathbb{R}^{d \times n_2}$ are matrices of coefficents of linear combinations. $U \in \mathbb{R}^{m \times d}$ and $V \in \mathbb{R}^{m \times d}$ certainly are matrices of evaluated linear combinations. $d=min(rank(X), rank(Y))$ is a number of pairs. Let us denote i-th rows of $W_x$, $W_y$ by $w_{xi}$ and suitably $w_{yi}$. For $i=1,\cdots,d$ the following condition must be met \cite{William}:
\begin{equation}\label{eq:cca2}
    (w_{xi}, w_{yi})=\argmax_{w_{xi}, w_{yi}} corr(Xw_{xi}, Yw_{yi}) = \argmax_{w_{xi}, w_{yi}} \left[\frac{u^{T}_iv_i}{\sqrt{u^T_iu_i}\sqrt{v^{T}_iv_i}}\right] 
\end{equation}
We should also remember that the correlation betweeen $u_i$ and $v_i$ where $i\neq j$ is equal to $0$.

Kettenring showed that \textit{CCA} is equivalent to simultaneously solving the following constrained optimization problem \cite{ChenLin}:
\begin{equation}\label{eq:cca3}
\begin{split}
    \min_{W_x, W_y} \norm{U-V}_{F}^2 = \min_{W_x, W_y} \norm{XW_x^T-YW_y^T}_{F}^2 \\ 
    \text{   subject to   } W_xX^TXW_x^T=W_yY^TYW_y^T=I   
\end{split}
\end{equation}

From the machine learning point of view, we can certainly treat $X$ and $Y$ as feature and label matrices and perform \textit{CCA} with them. Then we can obtain $U$ and $V$ matrices which are linear combinations of features and suitably labels in this case. The final step is to learn a linear regressor from $U$ to $V$. Unfortunately, such the algorithm is time-consuming. Let us notice that encoding and decoding data operations are slow, because $W_x$ and $W_y$ are not orthogonal (we cannot invert them by a simple transposition). The example algorithm, that perform \textit{CCA}, is shown in \Cref{app:cca}. 

Although there is no point in using pure \textit{CCA} to build a classifier, we can use methods which are based on \textit{CCA} concept. Such the algorithm will be discussed in the next section.

\subsection{CPLST algorithm}

\textit{CPLST} algorithm stands for \textit{Conditional Prinicipal Label Space Transformation} which means that only a label space will be transformed and then reduced. In contrast to \textit{PCA} approach, such the reduction is feature-aware. 

As it has been already mentioned, \textit{CCA} can be thought as an optimization problem of finding minimal prediction error (\Cref{eq:cca3}) under the constraint $W_xX^TXW_x^T=W_yY^TYW_y^T=I$. As long as we take into account only a label space, we can drop $W_xX^TXW_x^T=I$ constraint, because it is connected with a feature space which is not transformed. It is also worth enforcing orthogonalization of $W_y$  which simplifies a decoding of data. As a result we obtain the following problem:

\begin{equation}\label{eq:cplst1}
    \min_{W_x, W_y} \norm{XW_x^T-YW_y^T}_{F}^2  
    \text{  subject to   } W_yW_y^T=I   
\end{equation}
Let us notice that finding $W_x$ is simply the linear regression from $X$ to $YW_y^T$. It means that optimal $W_x$ is determined by the following equation:
\begin{equation}\label{eq:cplst2}
\begin{split}
    XW_x^T=YW_y^T \\
    W_x^T=X^PYW_y^T
\end{split}
\end{equation}
When optimal $W_x$ is inserted back into \Cref{eq:cplst1}, the optimization problem becomes:
\begin{equation}\label{eq:cplst3}
    \min_{W_yW_y^T=I} \norm{XX^PYW_y^T-YW_y^T}_{F}^2=\min_{W_yW_y^T=I} \norm{(XX^P-I)YW_y^T}_{F}^2
\end{equation}
For every matrix A $\norm{A}_{F}^2=tr(A^TA)$, so the above issue is equivalent to:
\begin{equation}\label{eq:cplst4}
    \min_{W_yW_y^T=I} tr(W_yY^T(I-H)YW_y^T)
\end{equation}
A matrix $H=XX^P$ is called a hat matrix for the linear regression. The presented approach is called \textit{Orthogonally constraint CCA}. In order to solve \Cref{eq:cplst4}, we can consider eigenvectors that correspond to the largest eigenvalues of $Y^T(H-I)Y$ (they can be computed by \textit{SVD} decomposition) \cite{ChenLin}.

As we see, \textit{OCCA} aims at minimizing the prediction error described by \Cref{eq:cplst1}, however an encoding error of a label space is not taken into account. In fact, a label space is transformed by eigenvectors captured from $Y^T(H-I)Y$ matrix. If we combine \textit{OCCA} approach with minimizing an encoding error, then we get the method called \textit{CPLST} which stands for \textit{Conditional Principal Label Space Transformation}. It can be modelled by the following formula:

\begin{equation}\label{eq:cplst5}
    \min_{W_x, W_y} \left(\norm{XW_x^T-YW_y^T}_{F}^2 + \norm{Y-YW_y^TW_y}_{F}^2\right)  
    \text{  subject to   } W_yW_y^T=I   
\end{equation}
If we use again \Cref{eq:cplst2} and the relation between a trace of matrix and its Frobenius norm, then we get:
\begin{equation}\label{eq:cplst6}
    \min_{W_yW_y^T} tr(W_yY^T(I-H)YW_y^T-W_y^TW_yY^TY-Y^TYW_y^TW_y+W_y^TW_yY^TYW_y^TW_y)  
\end{equation}
After eliminating a pair of $W_y$ and $W_y^T$ by cyclic permutation and combining the last three terms of \Cref{eq:cplst6}, we get:

\begin{equation}\label{eq:cplst6}
    \max_{W_yW_y^T} tr(W_yY^THYW_y^T)  
\end{equation}
The above problem can be solved analogously to \textit{OCCA} - by finding eigenvectors and eigenvalues of $Y^THY$. Besides balance between the prediction error and the encoding error, \textit{CPLST} is faster than \textit{OCCA}. It is an effect of decomposing $Y^THY$ instead of $Y^T(H-I)Y$. On the other hand, both the methods are slower than the algorithms based on the reduction by \textit{PCA}. It is caused by calculating a matrix $H$ what is time-consuming for large input data \cite{ChenLin}.

The algorithm is presented in \Cref{alg:cplst1} \cite{ChenLin}. As we see, the procedure is rather simple. First, \textit{SVD} decomposition of $Z^THZ$  is computed in order to transform a label space to a new space which has certainly lower dimension. A classifier is then built using the linear regression method. After making a prediction, we must remember about decoding data (return to an original label space).  

\begin{algorithm}
    \caption{Conditional Principal Label Space Transformation}\label{alg:cplst1}
    \begin{algorithmic}[1]
        \State Let $Z=[z_1 \cdots z_m]^T$ with $z_i=y_i-\bar{y}$
        \State Perform SVD on $Z^THZ$ to obtain $Z^THZ=U\Sigma V^T$ with $\sigma_{1} \geq \sigma_{2} \geq \cdots \sigma_{m}$. Let $V_h$ contain the left $h$ columns of $V$
        \State Encode $\{(x_i,z_i)\}^{m}_{i=1}$ to $\{(x_i,t_i)\}^{m}_{i=1}$ where $t_i=V^T_hz_i$
        \State Learn a multi-dimension regressor $r(x)$ from $\{(x_i,t_i)\}^{m}_{i=1}$ 
        \State Predict the label-set of an instance $x$ by $h(x)=round(V_hr(x)+\bar{y})$  
    \end{algorithmic}
\end{algorithm}


