
\chapter{Background}
\section{Notation}

Vectors are denoted by lowercase letters $w$, $x$ etc. while matrices by uppercase letters $W$, $X$ etc. Moreover $x^{(i)}$ is an $i-th$ row of matrix, $(x^T)^{(j)}$ is a $j-th$ column of matrix $X$ and $x_{ij}$ is an element in $i-th$ row and $j-th$ column. $X \in \mathbb{R}^{m \times n}$ symbol means that matrix $X$ has $m$ rows and $n$ columns. For $c \times d$  matrix $X \in \mathbb{R}^{c \times d}$, $\norm{X}_{F}$ is used for its Frobenius norm, $X^P$ is its pseudoinverse, $A^T$ is matrix transposed to $A$. 
ehere are also several functions used in this paper, ecspecially in pseudocode sections:
\begin{itemize}
\item \textit{tr(X)} is a trace of matrix $X$, 
\item \textit{randn(n,k)} generates $n \times k$ random matrix using normal distribution,
\item \textit{eig(X)} returns eigenvalues and eigenvectors of matrix $X$,
\item \textit{round(X)} returns $0$ if $x$ is closer to $0$ than $1$ else it returns $1$,
\item \textit{orthogonalize(X)} returns orthonormal basis of range space of matrix $X$,
\item \textit{inv(X)} inverses matrix $X$
\item \textit{head\_cols(X,h)} returns first $h$ left columns of matrix $X$,
\item \textit{push\_front\_column($w^T$,X)} inserts column $w^T$ into $X$ as first column.

\end{itemize}

\section{Formal problem definition}
Mutli-label classification is the problem of finding the following function: 

\begin{equation}\label{eq:def}
    f: [c_1, c_2, c_3, \cdots] \rightarrow [l_1, l_2, l_3, \cdots], \text{ } c_i \in \mathbb{R} \text{ } l_i \in \{0,1\} 
\end{equation}

As we see this classifier maps the vector of features into the vector of labels in contrast to standard classification, where a scalar is predicted. In order to face the problem and find desired function, we can use one of two main approaches: problem transformation methods and algorithm adaptation methods. In the first approach, a multi-label issue is considered as a set of binary classification problems - for each label a single-class classifier is trained. The second method focuses on adapting existing solutions for the standard classification to the multi-label. In other words, the problem is not divided into the simpler binary problems - it is seen as whole.

In this paper, the presented algorithms belong to the problem transformation methods approach. The base solution uses linear regression to predict specific labels. The rest of methods make an enhancement which rely on transformations of the feature and the label spaces. 

\section{Solution based on linear regression model}

In this section we focus on solution based on linear regression model. Regression analysis aims at studying, speaking generally, dependency between variables. 

Let $X \in \mathbb{R}^{m \times  ( n+1 )}$ be design matrix, $Y \in \mathbb{R}^{m \times k}$ label matrix, $\beta \in \mathbb{R}^{(n+1) \times k}$ matrix of coefficents of the regressor and $\epsilon \in \mathbb{R}^{m \times k}$ matrix of errors. The relation between these variables is described by formula:

\begin{equation}\label{eq:LR1}
    Y = XW + \epsilon 
\end{equation}
In order to estimate $\beta$ matrix, you might minimize sum of squared residuals (SSR), what can be written as a following formula:

\begin{equation}\label{eq:LR2}
    \hat{\beta} = \argmin_{\beta \in \mathbb{R}^{(n+1) \times k}}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{k}(y_{ij}-x^{(i)}(w^T)^{(j)})^2
\end{equation}
The solution of above optimization problem is equivalent to \cite{Weisberg}:

\begin{equation}\label{eq:LR3}
    \hat{\beta} = (X^TX)^{P}X^TY
\end{equation}
Although $X^TX$ matrix is squared, there is no certainty that it is ivertable. Therefore pseudoinverse must be used. There are of course prons and cons of this approach. First of all this model is simple and rather easy to implement - this is its main strength. Apart from that, it poses a base for different methods (some of them are described in this paper). On the other hand, there are strong assumptions which must be taken into account during costructing the model:
\begin{itemize}
    \item we assume linearity between feature variables and particular labels,
    \item feature values are fixed - they are not random,
    \item the error coefficents in matrix error are uncorrelated with each other - they have normal distribution,
    \item the number of examples should be higher than the number of features in the design matrix,
    \item SSR values are not correlated.
\end{itemize}

From the machine learning point of view, the main problem of linear regression is posed by a tendency to overfitting. However we can deal with that by regularization (Tikhonov regularization in this case). Then the problem might be described by a following formula:

\begin{equation}\label{eq:LR2}
    \hat{\beta} = \argmin_{\beta \in \mathbb{R}^{(n+1) \times k}}\left[\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{k}(y_{ij}-x^{(i)}(w^T)^{(j)})^2 + \lambda\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{k}w_{ij}^2\right]
\end{equation}
where $\lambda$ is a regularization parameter, which is a control on fitting parameters.
The solution, taking into account the regularization is then equivalent to \cite{Tikh}:

\begin{equation}\label{eq:LR3}
    \hat{\beta} = (X^TX+\lambda
        \begin{bmatrix}
        0 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 1
        \end{bmatrix}
    )^{-1}X^TY
\end{equation}
where the matrix following $\lambda$ is an $(n+1)\times(n+1)$ diagonal matrix.  

Both approaches certainly have similar complexity. There are two matrix operations used to create regressor: multiplying and inverting, which can be computed in $O(n^{2.373})$ time by algorithms based on Coppersmithâ€“Winograd method \cite{VVW}. Let us note that, matrix $X^TX \in \mathbb{R}^{(n+1) \times (n+1)}$ is inverted, so reduction of feature space (less columns) decreases time complexity. It is also worth to consider the reduction of $Y$ - for many labels it affects time-consumption too.   

The full learning algorithm based on linear regression is as shown in Algorithm \ref{alg:LR} 

\begin{algorithm}
    \caption{Linear regression based classifier}\label{alg:LR}
    \begin{algorithmic}[1]
    \State $\text{Let } Z \text{ be a copy of input feature matrix } X$
    \State $Z=push\_front\_column(ones, Z) \text{ where } ones=[1, 1, \cdots]^T$
    \State Let $I \in \mathbb{R}^{(n+1) \times (n+1)}$ be an identity matrix
    \State $I[0][0] = 0$
    \State $W=inv(Z^TZ+\lambda I)Z^TY$
    \State Predict label-set of an instance $x$ by $h(x)=round(xW)$ 
    \end{algorithmic}
\end{algorithm}


\section{Feature and label space reduction with PCA}

\subsection{Standard PCA algorithm}

One of possible methods which can be used to reduce dimension of data set is Principal Component Analysis. This reduction can be achieved by transforming to a new set of variables, the principal components, which are uncorrelated, and which are ordered so that the first few retain most of variation present in all of the original variables. Computation of the principal components reduces to the solution of an eigenvalue-eigenvector for a positive-semifdefinite symmetric matrix (covariance or correlation matrix) \cite{Jolliffe}.

Finding PCs can be obtained by \textit{Singular Value Decomposition}. Let $X \in \mathbb{R}_{m \times n}$ be an input matrix where $n$ is a number of variables. Moreover let us assume that $X$ is centered (a mean in each column is equal to $0$). Covariance matrix $C \in \mathbb{R}_{n \times n}$ is then equal to $\frac{X^TX}{n-1}$. Taking into account that $C$ is a symmetric matrix, its SVD decomposition is equivalent to:

\begin{equation}\label{eq:pca1}
    C=V\Sigma{V^T}
\end{equation}
where $V \in \mathbb{R}^{m \times m}$ is an orthogonal matrix of eigenvectors (called also principal axes), while $\Sigma \in \mathbb{R}^{m \times m}$ is diagonal matrix with eigenvalues in the decreasing order on its diagonal. 
Let us now consider SVD decomposition of $X$:

\begin{equation}\label{eq:pca2}
    X=V^{'}\Sigma^{'}{V^{'T}}
\end{equation}
It is easy to see that covariance matrix $C$ might be expressed by SVD decomposition of $X$:

\begin{equation}\label{eq:pca3}
    C=\frac{X^TX}{n-1} = \frac{(U^{\prime}\Sigma^{\prime}V^{\prime T})^T(U^{\prime}\Sigma^{\prime}V^{\prime T})}{n-1} = \frac{V^{\prime}\Sigma^{\prime}U^{\prime T}U^{\prime}\Sigma^{\prime}V^{\prime T}}{n-1} = V^{\prime}\frac{\Sigma^{\prime 2}}{n-1}V^{\prime T} 
\end{equation}
In fact, there is no need to use covariance matrix to compute its principals axes and its eigenvalues - SVD of $X$ matrix is enough \cite{Jolliffe}. In order to obtain new space for data (Prinicipal Components), the following equation can be used:

\begin{equation}\label{eq:pca4}
    PCs = XV = (U^{'}\Sigma^{'}V^{'T})V = U^{'}\Sigma^{'} 
\end{equation}
The reduction of data can be achieved by leaving first $h$ columns of $PCs$ matrix, where $h<n+1$.

PCA features and/or labels reduction can be simply combined with classifier based on linear regrssion model. At first a data are transformed into principal components to be reduced and then a regressor can be learnt from such encoded data. If a classifier is learnt from encoded label space, then a score of prediction must be decoded to an appropriate space. All required steps are presented in Algorithm \ref{alg:pca1}.

\begin{algorithm}
    \caption{Multi-dimension linear regressor with preliminary data reduction}\label{alg:pca1}
    \begin{algorithmic}[1]
    \Function{computePCA}{$X \in \mathbb{R}^{m \times n}$} 
        \State $\text{Let } Z=[z_1 \cdots z_m] \text{ with } z_i = x_i-\bar{x}$
        \State $\text{Perform svd on } Z \text{ to obtain } Z = U\Sigma{V^T}$
        \State $\text{return } (U,\Sigma,V)$
    \EndFunction
    \item[] 
    \State Let $X^{*}$ be a copy of $X$ and $Y^{*}$ be a copy of $Y$
    \If{reduceX == true}
        \State $(U_x, \Sigma_{x}, V_x) = computePCA(X,h1)$
        \State $X^{*}=head\_cols(X, h1)$
    \EndIf
    \If{reduceY == true}
        \State $(U_y, \Sigma_{y}, V_y) = computePCA(Y,h2)$
        \State $Y^{*}=head\_cols(Y, h2)$
    \EndIf
    \State Learn linear regressor $r(X^*)$ from $\{X^*,Y^*\}$
    \item[]
    \State Let $x^*$ be a copy of instance $x$
    \If{reduceX == true}
        \State $x^* = x^*V_x$
    \EndIf
    \If{reduceY == true}
        \State Predict the label-set of $x^*$ by $h(x^*)=round( r(x^*) * head\_rows(V_y^T, h2) + \bar{y})$ 
    \Else
        \State Predict the label-set of $x^*$ by $h(x^*)=round( r(x^*) )$ 
    \EndIf

    \end{algorithmic}
\end{algorithm}

Creating a regressor from reduced data has certainly less complexity than from original data, however PCA algorithm costs a time. The implementation based on SVD decomposition for $m\times{n}$ matrix is $O(m^2n + n^3)$, so it is hard to expect this method will work faster. It is also important to remember, that PCA might be understood as a lossy compression. For some data sets it might prevent an algorithm from being overfitted, but it might also decrease its quality - it is naturally connected to compression degree. 
One of idea to deal with complexity issue is to use faster PCA algorithm, which is not so accurate and gives an approximation. This kind of algorithm is described in the next section.     

\subsection{Randomized PCA algorithm}

In order to make computing PCs faster, we can use randomized version of algorithm. The full method is presented in Algorithm \ref{alg:rpca1}. 

\begin{algorithm}
    \caption{Randomized PCA}\label{alg:rpca1}
    \begin{algorithmic}[1]
    \Function{RPCA}{$k$, $X \in \mathbb{R}^{m \times n}$} 
        \State $(p,q) \leftarrow \text{(20,1)}$
        \State $Q \leftarrow \text{randn(n, k+p)}$
            \For{$i \in \{1,\cdots,q\}$}
            \State $\psi \leftarrow X^TXQ$
            \State $Q \leftarrow \text{orthogonalize($\psi$)}$
            \EndFor
            \State $F \leftarrow (X^TXQ)^T(X^TXQ)$
            \State $(V,\Sigma^2) \leftarrow \text{eig($F$,$k$)}$
            \State $V \leftarrow (X^TXQ)V\Sigma^P$
            \State $\text{return } (V, \Sigma)$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The algorithm takes two parameters on its input: data matrix $X \in \mathbb{R}^{m \times n}$ and input parameter $k$, which is responsible for compression degree (number of eigenvectors that are considered). The most imortant idea in this approach is to find the range for $X^TX$ matrix. 
The range for matrix $A$ can be understood as a collection of vectors $b$, which satisfy the equation $Ax=b$. Let us notice that eigenvectors might appear frequently as solutions of this equation. In fact, the range of $\psi$ will tend to be more aligned with the space spanned by the top eigenvectors of $X^TX$ \cite{Mineiro}. 

The randomized range finder begins in 4. line and computes an orthogonal basis for $\psi$. This operation is repeated $q$ times. We can certainly manipulate a parameter of $q$, however is should be rather small, beacues the complexity of computing an orthogonal basis is $O(nk^2)$. 
After finding a good aproximation for the principal subspace of $X^TX$, we optimize fully over that subspace and back out the solution. It is worth emphasising that the last steps, including eigendecoposition of $F \in \mathbb{R}^{(k+p) \times (k+p)}$, are cheap. It is caused by low dimensions of $F$ matrix \cite{Mineiro}.

We can now use the Randomized PCA algorithm with linear regressor, what is shown by Algorithm \ref{alg:rpca2} 

\begin{algorithm}
    \caption{Multi-dimension linear regressor with preliminary data reduction by RPCA}\label{alg:rpca2}
    \begin{algorithmic}[1]
    \State Let $X^{*}$ be a copy of $X$ and $Y^{*}$ be a copy of $Y$
    \If{reduceX == true}
    \State $(V_{x}, \Sigma_{x}) = RPCA(X,h1)$
        \State $X^{*}=X*V_{x}$
    \EndIf
    \If{reduceY == true}
        \State $(U_{y}, \Sigma_{y}) = RPCA(Y,h2)$
        \State $Y^{*}=Y*V_{y}$
    \EndIf
    \State Learn linear regressor $r(X^*)$ from $\{X^*,Y^*\}$
    \item[]
    \State Let $x^*$ be a copy of instance $x$
    \If{reduceX == true}
        \State $x^* = x^*V_x$
    \EndIf
    \If{reduceY == true}
        \State Predict the label-set of $x^*$ by $h(x^*)=round( r(x^*) * V_y^T)$ 
    \Else
        \State Predict the label-set of $x^*$ by $h(x^*)=round( r(x^*) )$ 
    \EndIf

    \end{algorithmic}
\end{algorithm}


\section{Label space reduction inspired by CCA}

Multi-label classifiers based on preliminary PCA compression of data have an essential disadventage - the reduction of label (feature) space is made independently of feature (label) space. In other words, label space is not aware of reduction feature space. One of possibilities, to deal with this issue, is to use statistical method called Canonical Correlation Analysis or rather methods inspired by CCA.   

\subsection{Canonical correlation analysis}

Let us assume you are given two data sets (matrices): $X \in \mathbb{R}^{m \times n_1}$ and $Y \in \mathbb{R}^{m \times n_2}$. Without loss of generality you suppose that we have substracted the mean in each column. CCA aims at finding linear combination $x$ coordinates that correlates well over the data with an linear combination of the $y$. In other words, you want to find best matched pair of linear combination of $X$ and $y$, which have the largest coefficent of correlation. You certainly do not need to stop there - you can ask for second-best pair, third-best pair and so on \cite{William}.

Formally, this problem can be expressed by two equations:
\begin{equation}\label{eq:cca1}
    U=XW_x^T,   V=YW_y^T    
\end{equation}
where $W_x \in \mathbb{R}^{d \times n_1}$, $W_y \in \mathbb{R}^{d \times n_2}$ are matrices of coefficents of linear combinations. $U \in \mathbb{R}^{m \times d}$ and $V \in \mathbb{R}^{m \times d}$ certainly are matrices of evaluated linear combinations. $d=min(rank(X), rank(Y))$ is the number o pairs, where $rank$ means column rank. Let us denote i-th rows of $W_x$, $W_y$, $U$ and $V$ by: $w_x^{(i)}$, $w_y^{(i)}$, $u^{(i)}$ and $v^{(i)}$. For $i=1,\cdots,d$ the following condition must be met \cite{William}:
\begin{equation}\label{eq:cca2}
    (w_x^{(i)}, w_y^{(i)})=\argmax_{w_x^{(i)}, w_y^{(i)}} corr[X(w_x^{(i)})^T, Y(w_y^{(i)})^T] = \argmax_{w_x^{(i)}, w_y^{(i)}} \left[\frac{u^{(i)}(v^{(i)})^T}{\sqrt{u^{(i)}(u^{(i)})^T}\sqrt{v^{(i)}(v^{(i)})^T}}\right] 
\end{equation}
You should also remember that the correlation betweeen $u^{(i)}$ and $v^{(j)}$ where $i\neq j$ is equal to $0$.

Kettenring showed that CCA is equivalent to simultaneously solving the following constrained optimization problem \cite{ChenLin}:
\begin{equation}\label{eq:cca3}
\begin{split}
    \min_{W_x, W_y} \norm{U-V}_{F}^2 = \min_{W_x, W_y} \norm{XW_x^T-YW_y^T}_{F}^2 \\ 
    \text{   subject to   } W_xX^TXW_x^T=W_yY^TYW_y^T=I   
\end{split}
\end{equation}

From the machine learning point of view, you can treat $X$ and $Y$ matrices as feature and label matrices and perform CCA on them. Then you can obtain $U$ and $V$ matrices, which certainly are linear combinations of features and suitably labels in this case. The final step is to learn linear regressor from $U$ to $V$. Unfortunately, this kind of algorithm is time-consuming. Let us notice that encoding and decoding data operations are slow, because $W_x$ and $W_y$ matrices are not orthogonal (you cannot simply invert those matrices). Although there is no point in using pure CCA to build classifier, you can use methods which base on CCA concept. This kind of algorithm will be discussed in the next section.

\subsection{CPLST algorithm}

CPLST algorithm stands for \textit{Conditional Prinicipal Label Space Transformation}, which means that only label space will be transformed and then reduced. In contrast to PCA approach, this kind of reduction is feature-aware. 

As it has been already mentioned, CCA can be considered as an optimization problem of finding minimal prediction error (equation \ref{eq:cca3}) under the constraint $W_xX^TXW_x^T=W_yY^TYW_y^T=I$. As long as you take into account only label space, you can drop $W_xX^TXW_x^T=I$ constraint, as it is connected with feature space, which is not transformed. It is also worth enforcing orthogonalization of $W_y$ matrix, which simplifies decoding of data. As a result you obtain the following problem:

\begin{equation}\label{eq:cplst1}
    \min_{W_x, W_y} \norm{XW_x^T-YW_y^T}_{F}^2  
    \text{  subject to   } W_yW_y^T=I   
\end{equation}
Let us notice that finding $W_x$ is just linear regression from $X$ to $YW_y^T$. It means that optimal $W_x$ is determined by following equation:
\begin{equation}\label{eq:cplst2}
\begin{split}
    XW_x^T=YW_y^T \\
    W_x^T=X^PYW_y^T
\end{split}
\end{equation}
When optimal $W_x$ is inserted back into (\ref{eq:cplst1}), the optimization problem becomes:
\begin{equation}\label{eq:cplst3}
    \min_{W_yW_y^T=I} \norm{XX^PYW_y^T-YW_y^T}_{F}^2=\min_{W_yW_y^T=I} \norm{(XX^P-I)YW_y^T}_{F}^2
\end{equation}
For every matrix A $\norm{A}_{F}^2=tr(A^TA)$, so above issue is equivalent to:
\begin{equation}\label{eq:cplst4}
    \min_{W_yW_y^T=I} tr(W_yY^T(I-H)YW_y^T)
\end{equation}
Matrix $H=XX^P$ is the hat matrix for linear regression. The presented approach is called \textit{Orthogonally constraint CCA}. In order to solve (\ref{eq:cplst4}), you can consider the eigenvectors that correspond to the largest eigenvalues of $Y^T(H-I)Y$ (it can be reached by SVD decomposition) \cite{ChenLin}.

As you see, OCCA aims at minimizing the prediction error (\ref{eq:cplst1}), however an encoding error of label space is not taken into account. In fact, label space is transformed by eigenvectors captured from $Y^T(H-I)Y$ matrix. If you combine OCCA approach with minimizing the encoding error, then you get CPLST algorithm. It can be described by following formula:

\begin{equation}\label{eq:cplst5}
    \min_{W_x, W_y} \left(\norm{XW_x^T-YW_y^T}_{F}^2 + \norm{Y-YW_y^TW_y}_{F}^2\right)  
    \text{  subject to   } W_yW_y^T=I   
\end{equation}
If you use again (\ref{eq:cplst2}) and relation between trace of matrix and its Frobenius norm, then you get:
\begin{equation}\label{eq:cplst6}
    \min_{W_yW_y^T} tr(W_yY^T(I-H)YW_y^T-W_y^TW_yY^TY-Y^TYW_y^TW_y+W_y^TW_yY^TYW_y^TW_y)  
\end{equation}
After eliminating a pair of $W_y$ and $W_y^T$ by cyclic permutation and combining the last three terms of (\ref{eq:cplst6}), you get:

\begin{equation}\label{eq:cplst6}
    \max_{W_yW_y^T} tr(W_yY^THYW_y^T)  
\end{equation}

The above problem can be solved analogously to OCCA - by finding eigenvectors and eigenvalues of $Y^THYT$ The resulting algorithm is presented in Algorithm \ref{alg:cplst1} \cite{ChenLin}. Besides balance between the prediction error and the encoding error, CPLST is faster than OCCA. It is an effect of decomposing $Z^THZ$ matrix instead of $Z^T(H-I)Z$. On the other hand, both methods are slower than algorithms based on PCA reduction. It is caused by calculating $H$ matrix, which is time-consuming for large input data.

\begin{algorithm}
    \caption{Conditional Principal Label Space Transformation}\label{alg:cplst1}
    \begin{algorithmic}[1]
        \State Let $Z=[z_1 \cdots z_m]^T$ with $z_i=y_i-\bar{y}$
        \State Perform SVD on $Z^THZ$ to obtain $Z^THZ=U\Sigma V^T$ with $\sigma_{1} \geq \sigma_{2} \geq \cdots \sigma_{m}$. Let $V_h$ contain the left $h$ columns of $V$
        \State Encode $\{(x_i,z_i)\}^{m}_{i=1}$ to $\{(x_i,t_i)\}^{m}_{i=1}$ where $t_i=V^T_hz_i$
        \State Learn a multi-dimension regressor $r(x)$ from $\{(x_i,t_i)\}^{m}_{i=1}$ 
        \State Predict the label-set of an instance $x$ by $h(x)=round(V_hr(x)+\bar{y})$  
    \end{algorithmic}
\end{algorithm}


