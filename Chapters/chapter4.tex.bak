
\chapter{Results}

The computational experiment consisted off two parts: measuring of the accuracy of prediction and the time of training a classifier. 

In the first part of the experiment (let us call it \textit{the accuracy experiment}) the accuracy of prediction was measured by typical metrics for multi-label classification -- a detailed description of these metrics can be found in \Cref{sec:metrics}. It is worth emphasising that the computation was performed with six data sets which vary a number of instances and attributes. A particular training set was devided into equal halves: the first one was used to train a classifier and the second one was used to evaluate this classifier. Before starting the whole procedure, a specific training set was shuffled a few times in order to choose instances randomly for training process. The results of this part of the experiment are shown in \Cref{sec:exp_a}. They are gathered in tables which present a value of a specific metric for a particular algorithm and a particular data set. There are also charts which present an influence of the reduction degree on values of the metrics.   

The second part of the experiment was connected with measuring the time of training the classifier in a single-threaded and a multi-threaded environment as well (let us call it \textit{the efficiency experiment}). It is worth mentioning that this test was performed on the bookmarks data set with changeable number of instances. The results of this part of the experiment are shown in \Cref{sec:exp_e} and are visualized by charts which present a relation between the training time and the number of instances. 

The computational experiment involved the following algorithms:
\begin{itemize}
    \item CPLST which is described in \Cref{sec:cplst}, 
    \item LR which is described in \Cref{sec:lr},
    \item OCCA which is described in \Cref{sec:cplst},
    \item LRWithPCA which denotes linear regression with PCA transformation on a label space (\Cref{sec:pca}),
    \item LRWithRandomPCA which denotes linear regression with randomized PCA compression on a label space (\Cref{sec:rpca}).
\end{itemize}
All of the tested approaches used linear regression with Tikhonov regularization. The regularization coefficient was equal to $7.0$. This value was set up experimentally and does not have to be optimal for different data sets. The algorithms with reduction discarded $40\%$ of variables in new linear spaces (reduction degree was equal to $0.6$).

The relation between the reduction degree and values of the metrics for LR should be a constant function, however, we can observe little fluctuations in \Crefrange{fig:scene}{fig:bookmarks}. It is an effect of shuffling instances. 


\section{Experiment environment}
\subsection{Data sets used in the experiment}

All of the data sets used in the experiment come from \textit{Mulan}\footnote{\bibentry{Mulan}} project web page. The sizes of sets are differentiated in order to check behaviours of the algorithms for a low, a medium and a high number of instances. The detailed information about data sets are presented in \Cref{tab:exp1}. 

\begin{table}[h]
\centering
\caption{Statistics and details of data sets used in the experiment.}
\label{tab:exp1}
    \begin{tabular}{l|c|r|r|r}
    name & domain & instances & features & labels \\ \hline \hline
    scene & image &  2407  & 294 & 6 \\   
    yeast & biology & 2417 & 103 & 14 \\
    bibtex & audio & 7395 & 1836 & 159 \\
    corel16k (10 samples) & images & 13811 $\pm$ 87 & 500 & 161 $\pm$ 9\\
    EUR-Lex (directory codes) & text & 19348 & 5000 & 412 \\
    bookmarks & text & 87856 & 2150 & 208
    \end{tabular}
\end{table}

\subsection{Parameters of the computer used in the experiment}

The experiment was performed on one of the available instances (m4.4xlarge) provided by \textit{Amazon Web Services}. It has 64 GB of memory and \textit{Intel Xeon® E5-2676 v3 (Haswell)} processor. Some details connected with CPU can be found in \Cref{tab:cpu}. The operating system, used in the experiment, was \textit{Ubuntu 14}.

\begin{table}[h]
\centering
\caption{Extra information about Intel Xeon® E5-2676 v3 (Haswell).}
\label{tab:cpu}
    \begin{tabular}{l|l}
    & \\ \hline \hline
    Cache & 20 MB SmartCache \\
    Bus speed & 8 GT/s QPI \\
    Instruction set & 64-bit \\
    Number of cores & 8 \\
    Number of threads & 16 \\
    Processor base frequency & 2.4 GHZ \\
    Intel$^{®}$ Turbo Boost Technology$^{‡}$ & 2.0 \\
    Intel$^{®}$ Turbo vPro Technology$^{‡}$ & yes \\ 
    Intel$^{®}$ Turbo Hyper-Threading Technology$^{‡}$ & yes \\
    Intel$^{®}$ Turbo Virtualization Technology$^{‡}$ & yes
    \end{tabular}
\end{table}

The library was compiled by GCC 4.9 with the third optimization level (\textit{-O3} flag). The build process certainly takes more time and memory due to inlining functions or loop unrolling etc., however a performance of code is faster. We can find more information about the optimization flags in GCC documentation.\footnote{\bibentry{Opt}} It is also worth mentioning that OpenBLAS was used instead of standard BLAS API, in order to enhance the efficiency of linear algebra operations and to use multithreading. 

\subsection{Metrics of accuracy}\label{sec:metrics}

The accuracy of the algorithms was measured by the standard metrics used in multi-label classification. In order to define these metrics, we will use the following terminology:
\begin{itemize}
    \item \textit{True positive (TP)} -- a number of labels which were correctly predicted,
    \item \textit{True negative (TN)} -- a number of labels which were correctly rejected,
    \item \textit{False positive (FP)} -- a number of labels which were incorrectly predicted,
    \item \textit{False negative (FN)} -- a number of labels which were incorrectly rejected.
\end{itemize}

\subsubsection{Precision and recall} 

\textit{Precision} and \textit{recall} are very similar measures. Precision is a fraction of a number of labels predicted correctly to a number of all predicted labels, while recall (also called \textit{sensitivity}) is a fraction of a number of labels predicted correctly to a number of all appropriate labels. They can be simply described by the following formulas:  
\begin{equation}
\label{eq:exp2}
precision=\frac{TP}{TP+FP}
\end{equation}
\begin{equation}
\label{eq:exp3}
recall=\frac{TP}{TP+FN}
\end{equation}
Although their definitions are always the same, they can be computed by three different methods:
\begin{itemize}
\item Macro-average -- in this method we compute precision and recall for each category independently. Finally we take the arithmetic average of these values.
\item Micro-average -- this method is the most intuitive. We simply sum up true positives, false positives and false negatives for a whole set and then apply them to get the metric. 
\item Instance-average -- in this approach we compute precision and recall for each instance. In the next step, similarly to Macro-average, the arithmetic average is calculated.
\end{itemize}

Unfortunately none of these methods is more significant than the others, so the best way is to use every approach in order to compare the accuracy of classifiers.  

\subsubsection{F1 score}

This metric is simply the harmonic average of precision and recall. Its value is certainly dependent on a chosen method of calculating precision and recall. As a result, we define Micro-, Macro-, Instance-average F1 score. We can express it by the following formula:
\begin{equation}\label{eq:f1}
F1=\frac{2*precision*recall}{precision+recall}   
\end{equation}

\subsubsection{Hamming loss}

This metric\footnote{\bibentry{Loss}} can be defined as a fraction of number of wrong labels ($FN+FN$) to a total number of labels. It is described by the following formula:
\begin{equation}
\label{eq:exp1}
    HammingLoss(z_i, y_i)=\frac{1}{m}\sum\limits_{i=1}^{m}\frac{xor(z_i,y_i)}{k}
\end{equation}
where $m$ is a number of instances, $k$ is a number of labels, $y_{i}$ is an appropriate vector of labels  and $z_{i}$ is a predicted vector of labels. Let you notice that, unlike the other metrics, we want Hamming loss to be as low as possible.  

\section{Presentation of results}

\subsection{The accuracy experiment}\label{sec:exp_a}

\subsubsection{The scene data set}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the scene data set.}
\label{tab:exp2}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/scene.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the scene data set.}
\label{fig:scene}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp5}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.35]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/scene_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/scene_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/scene_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/scene_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/scene_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp6}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        width=7cm,
        xmax=1,
        ymax=1.5]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/scene_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/scene_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/scene_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/scene_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/scene_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp7}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        width=7cm,
        xmax=1,
        ymax=1.5]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/scene_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/scene_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/scene_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/scene_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/scene_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp8}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        width=7cm,
        xmax=1,
        ymax=1.3]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/scene_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/scene_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/scene_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/scene_3.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/scene_3.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}


\end{figure}


\subsubsection{The yeast data set}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the yeast data set.}
\label{tab:exp3}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/yeast.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the yeast data set.}
\label{fig:yeast}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp9}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.25]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/yeast_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/yeast_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/yeast_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/yeast_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/yeast_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp10}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.5]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/yeast_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/yeast_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/yeast_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/yeast_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/yeast_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp11}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.8]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/yeast_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/yeast_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/yeast_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/yeast_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/yeast_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp12}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        width=7cm,
        xmax=1,
        ymax=0.8]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/yeast_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/yeast_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/yeast_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/yeast_3.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/yeast_3.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\end{figure}

\subsubsection{The bibtex data set.}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the bibtex data set}
\label{tab:exp4}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/bibtex.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the bibtex data set.}
\label{fig:bibtex}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp13}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.02]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bibtex_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bibtex_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bibtex_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bibtex_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bibtex_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp14}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.45]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bibtex_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bibtex_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bibtex_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bibtex_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bibtex_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp15}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.75]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bibtex_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bibtex_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bibtex_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bibtex_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bibtex_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp16}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        width=7cm,
        xmax=1,
        ymax=0.65]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bibtex_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bibtex_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bibtex_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bibtex_3.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bibtex_3.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}


\end{figure}


\subsubsection{The corel16k data set}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the corel16k data set.}
\label{tab:exp5}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/Corel16k001.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the corel16k data set.}
\label{fig:corel16k}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp17}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.02]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/Corel16k001_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/Corel16k001_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/Corel16k001_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/Corel16k001_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/Corel16k001_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp18}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.05]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/Corel16k001_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/Corel16k001_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/Corel16k001_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/Corel16k001_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/Corel16k001_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp19}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        ylabel style={yshift=0.4cm},
        width=7cm,
        xmax=1,
        ymax=0.15]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/Corel16k001_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/Corel16k001_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/Corel16k001_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/Corel16k001_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/Corel16k001_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp20}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        ylabel style={yshift=0.4cm},
        width=7cm,
        xmax=1,
        ymax=0.13]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/Corel16k001_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/Corel16k001_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/Corel16k001_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/Corel16k001_3.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/Corel16k001_3.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}


\end{figure}

\subsubsection{The EUR-Lex data set}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the EUR-Lex data set.}
\label{tab:exp6}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/eurlex.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the EUR-Lex data set.}
\label{fig:eurlex}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp21}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.05]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/eurlex_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/eurlex_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/eurlex_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/eurlex_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/eurlex_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp22}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        ylabel style={yshift=0.4cm},
        width=7cm,
        xmax=1,
        ymax=0.15]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/eurlex_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/eurlex_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/eurlex_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/eurlex_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/eurlex_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp23}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.4]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/eurlex_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/eurlex_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/eurlex_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/eurlex_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/eurlex_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp24}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        width=7cm,
        xmax=1,
        ymax=0.65]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/eurlex_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/eurlex_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/eurlex_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/eurlex_3.csv};
        \addlegendentry{OCCA};
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/eurlex_3.csv};
        \addlegendentry{LR}

    \end{axis}
\end{tikzpicture}
\end{subfigure}


\end{figure}

\subsubsection{The bookmarks data set}

\begin{table}[H]
\centering
\caption{Accuracy of methods for the bookmarks data set.}
\label{tab:exp7}
\pgfplotstabletypeset [
    col sep=semicolon,
    columns/Method/.style={string type, column type={c|}},
    columns/Macro F1 score/.style={column type={c|}},
    columns/Micro F1 score/.style={column type={c|}},
    columns/Instance F1 score/.style={column type={c|}},
    columns/Hamming loss/.style={column type={c|}},
    every head row/.style={after row=\hline\hline}
]{figures/bookmarks.csv}
\end{table}

\begin{figure}[H]
%\centering
\caption{Relation between reduction degree and accuracy for the bookmarks data set.}
\label{fig:bookmarks}

\begin{subfigure}{.5\textwidth}
\caption{Hamming loss.}
\label{fig:exp25}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Hamming loss},
        width=7cm,
        xmax=1,
        ymax=0.01]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bookmarks_0.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bookmarks_0.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bookmarks_0.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bookmarks_0.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bookmarks_0.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Macro F1 score.}
\label{fig:exp26}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Macro F1 score},
        ylabel style={yshift=0.4cm},
        width=7cm,
        xmax=1,
        ymax=0.13]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bookmarks_1.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bookmarks_1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bookmarks_1.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bookmarks_1.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bookmarks_1.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\caption{Micro F1 score.}
\label{fig:exp27}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont}, 
        xlabel={Degree of reduction},
        ylabel={Micro F1 score},
        width=7cm,
        xmax=1,
        ymax=0.3]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bookmarks_2.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bookmarks_2.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bookmarks_2.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bookmarks_2.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bookmarks_2.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Instance F1 score.}
\label{fig:exp28}
\begin{tikzpicture}
    \begin{axis}[legend pos=north east,
        legend style={font=\fontsize{7}{5}\selectfont},
        xlabel={Degree of reduction},
        ylabel={Instance F1 score},
        width=7cm,
        xmax=1,
        ymax=0.25]
        \addplot table [x=k, y=LRWithPCA, col sep=semicolon] {figures/bookmarks_3.csv};
        \addlegendentry{LRWithPCA}
        \addplot table [x=k, y=CPLST, col sep=semicolon] {figures/bookmarks_3.csv};
        \addlegendentry{CPLST}
        \addplot table [x=k, y=LRWithRandomPCA, col sep=semicolon] {figures/bookmarks_3.csv};
        \addlegendentry{LRWithRandomPCA}
        \addplot table [x=k, y=OCCA, col sep=semicolon] {figures/bookmarks_3.csv};
        \addlegendentry{OCCA}
        \addplot table [x=k, y=LR, col sep=semicolon] {figures/bookmarks_3.csv};
        \addlegendentry{LR}
    \end{axis}
\end{tikzpicture}
\end{subfigure}


\end{figure}

\newpage

\subsection{The efficiency experiment}\label{sec:exp_e}

\subsubsection{Single-threaded performance}

\begin{figure}[H]
\centering
\caption{Efficiency of single-threaded LR and RPCA classifiers.}
\label{fig:exp29}
\begin{tikzpicture}[scale=0.88]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax={60}]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LR1.csv};
        \addlegendentry{LR}
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LRWithRandomPCA1.csv};
        \addlegendentry{LRWithRPCA}
    \end{axis}
\end{tikzpicture}
\end{figure}

\begin{figure}[H]
\centering
\caption{Efficiency of single-threaded OCCA and CPLST.}
\label{fig:exp30}
\begin{tikzpicture}[scale=0.88]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax={1500}]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/CPLST1.csv};
        \addlegendentry{CPLST}
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/OCCA1.csv};
        \addlegendentry{OCCA}
    \end{axis}
\end{tikzpicture}
\end{figure}

\begin{figure}[h]
\centering
\caption{Efficiency of single-threaded LRWithPCA.}
\label{fig:exp31}
\begin{tikzpicture}[scale=0.88]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax={5000}]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LRWithPCA1.csv};
        \addlegendentry{LRWithPCA}
    \end{axis}
\end{tikzpicture}
\end{figure}

\subsubsection{Multi-threaded performance}

\begin{figure}[H]
\centering
\caption{Efficiency of multi-threaded LR and RPCA classifiers.}
\label{fig:exp32}
\begin{tikzpicture}[scale=0.90]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax=16]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LR.csv};
        \addlegendentry{LR}
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LRWithRandomPCA.csv};
        \addlegendentry{LRWithRPCA}
    \end{axis}
\end{tikzpicture}
\end{figure}

\begin{figure}[H]
\centering
\caption{Efficiency of multi-threaded OCCA and CPLST.}
\label{fig:exp33}
\begin{tikzpicture}[scale=0.90]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax=300]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/CPLST.csv};
        \addlegendentry{CPLST}
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/OCCA.csv};
        \addlegendentry{OCCA}

    \end{axis}
\end{tikzpicture}
\end{figure}

\begin{figure}[H]
\centering
\caption{Efficiency of multi-threaded LRWithPCA.}
\label{fig:exp34}
\begin{tikzpicture}[scale=0.90]
    \begin{axis}[legend pos=north east,
        xlabel={number of instances},
        ylabel={time [s]},
        ymax=2500]
        \addplot table [x=instance, y=time, col sep=semicolon] {figures/LRWithPCA.csv};
        \addlegendentry{LRWithPCA}
    \end{axis}
\end{tikzpicture}
\end{figure}
