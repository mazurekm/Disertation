\chapter{Conclusion}

\section{Summary of quality experiment}

The quality of classification for all the methods is strongly dependent on a training set and a metric which is taken into account. The most "difficult" training set is \textit{corel16k}, even though it is not the biggest set - its number of features, number of labels and number of instances as well are lower than in case of \textit{bookmarks} or \textit{EUR-Lex} data sets. Besides \textit{Hamming loss} metric, which is satisfying, the rest of them is levelled of $10^2$ (Table \ref{tab:exp5}) and Figure \ref{fig:corel16k}) what is a very low score. As we see, a nature and a domain of data set also affect the results. On the other hand, it is hard to find the correlation between the domain and the scores. \textit{Scene} data set, as well as \textit{corel16k}, is connected with images, but the results are far better for the first one (Table \ref{tab:exp3}). It is worth mentioning that high results have been also achieved for \textit{yeast} set - its domain is biology.

The most effective approach, when it comes about quality and regardless of the metric, is \textit{LR}. This is not certainly suprising - the rest of methods use compression which causes fall of quality metrics. Among the algorithms which use compression the best results are given by \textit{CPLST}. This is also rather predictible, because the reduction of a label space is feature awared, so it is done dependently on a feature sapce. Nevertheless, we should remember that all the approaches uses linear regressor to create a classifier. Thus, behaviours of the algorithms are rather similar for particular dataset. For instance the characteristic features of all the methods are low score of f1 macro average and satisfying score of hamming loss for a majority of datasets used in the experiment. 

Figure \ref{fig:enron} - Figure \ref{fig:bookmarks} show the relation between the reduction degree and a value of a specific metric. As we see the lower reduction degree we use, the worse scores we get in almost all cases. An interesting anomaly is \textit{EUR-Lex} dataset. In this particular case \textit{hamming loss} increases regularly with a growth of reduction degree value. The situation is similar when it comes about f1 micro average (it should grow but it decreases). The domain of \textit{EUR-Lex} is text - this set is connected with \textit{TF-IDF} representation of documents. It is possible that most of the features are useless and make a noise in the data - it could explain the anomaly.  

\section{Summary of efficency experiment}

All the methods, which took part in the experiment, perform faster in a multi-threaded environment. The difference is noticeable - for some algorithms the computation is even four times faster. It is certainly the result of efficient linear operations (matrix multiplication etc.) which are suitable for parallelization.

The best approach, when it comes about time-consumptions, 

\section{Future research implications}

The solution 
