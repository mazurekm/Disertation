\chapter{Conclusion}

\section{Discussing quality experiment results}

The quality of classification for all the methods is strongly dependent on the training sets and the metric which is taken into account. The most 'difficult' training set is \textit{corel16k}, even though it is not the biggest set - its number of features, number of labels and number of instances as well are lower than in case of \textit{bookmarks} or \textit{EUR-Lex} data sets. Besides \textit{Hamming loss} metric, which is satisfying, the rest of them is levelled of $10^{-2}$ (\Cref{tab:exp5} and \Cref{fig:corel16k}) what is a very low score. As we see, a nature and a domain of data set also affect the results. On the other hand, a domain is not a dominant factor. \textit{Scene} data set, as well as \textit{corel16k}, is connected with images, but the results are far better for the first one (\Cref{tab:exp3}). It is worth mentioning that high scores were also reached for \textit{yeast} set - its domain is 'biology'.

The most effective approach, when it comes about quality and regardless of the metrics, is \textit{LR}. This is not certainly suprising - the rest of the methods use compression which causes a fall of the quality metrics. Among the algorithms which use compression, the best results were given by \textit{CPLST}. This is also rather predictible, because the reduction of a label space is feature awared unlike \textit{PCA} compression. Nevertheless, we should remember that all the approaches use a linear regressor to create a classifier. Thus behaviours of the algorithms are rather similar for particular datasets. For instance the characteristic features of all the methods are a low score of \textit{Macro-average F1} and a satisfying score of \textit{Hamming loss} for the majority of the datasets used in the experiment. 

\Crefrange{fig:enron}{fig:bookmarks} show the relation between a value of a reduction degree and a value of specific metrics. As we see the lower reduction degree we use, the worse scores we get in almost all cases. An interesting anomaly appears in \textit{EUR-Lex} dataset. In this particular case \textit{Hamming loss} increases regularly with a growth of a reduction degree value. The situation is similar when it comes about \textit{Micro-average F1} (it should grow but it decreases). The domain of \textit{EUR-Lex} is 'text' - this set is connected with \textit{TF-IDF} representation of documents. It is possible that most of the features are useless and make a noise in the data - it could explain the anomaly.  

\section{Discussing efficency experiment results}

All the methods, which took part in the experiment, perform faster in the multi-threaded environment. The difference is noticeable - for some algorithms a computation is even four times faster. It is certainly the result of efficient matrix operations (matrix multiplication etc.) which are suitable for parallelization. It is also obvious that time-consumption grows with a number of instances.

The best approach, when it comes about time-consumption, is the linear regression (\Cref{fig:exp29} and \Cref{fig:exp32}). Although the other methods create a classifier from reduced data, the time cost of such the compression is too high. However, the results reached by \textit{LRWithRPCA} are not much worse than in case of \textit{LR}. Unlike the rest of the approaches based on preliminary compression, \textit{LRWithRPCA} doses not use \textit{SVD} decomposition. This fact explains why this method is much faster - it is worth reminding that \textit{SVD} has the complexity of $O(m^2n+n^3)$. 

\Cref{fig:exp31,fig:exp34} show that the slowest approach is \textit{LRWithPCA}. We should remember that only a label space was compressed in the experiment - compressing a feature space is too time-consuming in case of \textit{LRWithPCA}. As we see, this implementation is useless for massive data. 

\textit{CPLST} and \textit{OCCA} (\Cref{fig:exp30,fig:exp33}) have almost the same time complexity. It is certainly not suprising - both the methods process matrices of the same size in a very similar way.

\section{Future research implications}

The efficency experiment has shown that parallelizing of computation has huge influence on time-consumption. Therefore it is sensible to implement the algorithms for environments which use GPU to process. This will probably improve the efficiency for all the methods. It is also worth checking if an involvement of greater amount of cores (threads) in processing decreases time-consumption.  

Among the tested algorithms there are two approaches which can be rejected: \textit{LRWithPCA} and \textit{OCCA}. The first one is definitely too slow and is also not satisfying when it comes about the quality. In fact, its random version gives similar quality results (sometimes even better). The second one is exactly as fast as \text{CPLST} algorithm, but reached worse scores in the quality experiment. 

The most suprising aspect of the research is the result achieved by \textit{LR} algorithm in both the experiments - especially in the second one. The transformations of feature and labels spaces have aimed at decreasing input matrices for the linear regressor. Unfortunately these transformations are too time-consuming towards building the regressor. However, the time achieved by \textit{LRWithRPCA} is not much worse. Moreover it is possible to manipulate the specific parameters of this method what can help obtain better results. All in all there is a lot to investigate in this particular approach.   

Even though \textit{CPLST} is not as efficient as \textit{LR} and \textit{LRWithRPCA}, this approach should also be studied - more generally, methods inspired by \textit{CCA} should be studied. Let us remind that the algorithm reached better results than the algorithms which use \textit{PCA} in the quality experiment. In this specific implementation of \textit{CPLST}, a linear regressor is used to build a final classifier - it is sensible to check different solutions, for example a logistic regressor etc. It is also sensible to check how some different regressor would cooperate with \textit{RandomPCA} algorithm.

\section{Summery}

The scope of the thesis has involved fast implementations of multi-label classification algorithms based on preliminary transformations of feature and label spaces. This goal has been achieved - the efficency experiment showed that the methods 'learn' fast, even from massive data. However, as it has been mentioned in the previous section, there are still many approaches to test.  

It is significant to remeber that such a good efficency is the result of the calculation parallelizing which were possible by the fast linear algebra libraries. It is also worth noting that the best scores were reached by the standard linear regression algorithm which is rather a simple approach. In fact, it is sensible to consider simpler models which can be effectively parallelized and improve them instead of developing complicated methods. 

The implementations of the algorithms, discussed in this paper, was written in \textit{C++11} programming language and became a part of the library called \textit{MLCPACK}. The library was organised in a flexible way (\textit{strategy} design pattern) and provides the interface which can be used to add new algorithms. Besides the algorithms, the library contains additional tools, such as \textit{ARFF} parser or quality evaluators. It is woth mentioning that \textit{MLCPACK} is prepared for performing in a multi-threaded environment.
