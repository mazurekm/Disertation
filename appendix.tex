\begin{appendices}

\chapter{Proofs and facts}

\section{Least squares for multiple regression}
\label{app:least}

Given a design matrix $X \in \mathbb{R}^{m \times n}$ and a vector of dependent variables $y \in \mathbb{R}^{n}$, we want to find $w$, such as:
\begin{equation}
    \hat{w} = \argmin_{w \in \mathbb{R}^{n}}\sum\limits_{i=1}^{m}(y_i-x_iw^T_i)^2 = \argmin_{w \in \mathbb{R}^{n}}(y-Xw)^T(y-Xw)
\end{equation}
The above optimization problem can be simply transformed to the following equation:
\begin{equation}\label{eq:least1}
    \nabla_w \frac{1}{2}(y-Xw)^T(y-Xw) = 0
\end{equation}
As we see, we need to calculate the derivative of $\frac{1}{2}(y-Xw)^T(y-Xw)$ in respect to $w$. In order to reach it, we can use a ralation between a derivative of matrix and its trace. Fist of all, let us note a few facts about trace of matrix and its connection with a derivative:

\begin{enumerate}
    \item $tr(A+B) = tr(A)+tr(B)$
    \item $tr(AB) = tr(BA)$
    \item $tr(ABC) = tr(CAB) = tr(BCA)$
    \item $\nabla_Atr(AB) = B^T$
    \item $tr(A)=tr(A^T)$
    \item if $a \in \mathbb{R}$ then $tr(a)=a$
    \item $\nabla_Atr(ABA^TC) = CAB+C^TAB^T$
\end{enumerate}
Calculating a dot product in \ref{eq:least1} and using the property (6.) of trace operator, we obtain:
\begin{equation}\label{eq:least2}
    \frac{1}{2} \nabla_w tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty) = 0
\end{equation}
Next, using the properties (1.), (3.) and again (6.), \ref{eq:least2} is equivalent to:
\begin{equation}\label{eq:least3}
    \frac{1}{2}[\nabla_wtr(ww^TX^TX)-\nabla_wtr(y^TXw)-\nabla_wtr(y^TXw))] = 0
\end{equation}
The derivative of $y^Ty$ is $0$, because it does not depend on $w$. Thus it has been removed. Let us also notice that:
\begin{equation}\label{eq:least4}
    \nabla_wtr(ww^TX^TX) = \nabla_wtr(wIw^TX^TX) = X^TXwI + XTXwI = X^TXw+X^TXw
\end{equation}
because of (7.) property of trace operator and:
\begin{equation}\label{eq:least5}
    \nabla_wtr(y^TXw) = X^Ty
\end{equation}
because of (4.) property.
Now we can simply use \ref{eq:least4} and \ref{eq:least5} expressions in \ref{eq:least3} and calculate the desired derivative:
\begin{equation}\label{eq:least6}
    \frac{1}{2}[X^TXw+X^TXw-X^Ty-X^Ty] = 0
\end{equation}
Finally, we obtain $w$ from the equation \ref{eq:least6}:
\begin{equation}
    w=(X^TX)^{-1}X^Ty
\end{equation}

\newpage
\section{Eigenvalues and eigenvectors of matrices}
\label{app:eigen}

Let matrix $T \in \mathbb{R}^{m \times n}$ be a linear transformation matrix - such a matrix can express primary linear transformations, for instance rotation, scaling, transposition, reflection, etc. If there are a column vector $v$ and a scalar $\lambda$, such as:
\begin{equation}\label{eq:eigen1}
    Tv=\lambda{v}
\end{equation}
then we call them an eigenvector and suitably an eigenvalue of matrix $T$. Intuitively, the  eigenvector $v$ of the matrix $T$ is transformed by the $T$ into itself - its direction does not change. The eigenvalue can be then intepreted as a scale factor. Formally, the eigenvectors and the eigenvalues chracterize an endomorphism of a particular linear space.  

The equation \ref{eq:eigen1} can be certainly stated equivalenty as:

\begin{equation}\label{eq:eigen2}
    (T-\lambda{I})v=0
\end{equation}
where $I$ is an identity matrix. Let us notice that \ref{eq:eigen2} has a solution if and only if the determinant of the matrix $(T-\lambda{I})$ is equal to zero. As a result, we can find eigenvalues by solving the following equation:
\begin{equation}\label{eq:eigen3}
    det(T-\lambda{I}) = (\lambda_1-\lambda)(\lambda_2-\lambda)\cdots(\lambda_n-\lambda) = 0
\end{equation}
The equation \ref{eq:eigen3} is called a characteristic equation, while its left-hand side is called characteristic polynomial. In practise this method is not used to calculate eigenvectors and eigenvalues. The most popular algorithm is \textit{power iteration} which is also called \textit{Von Mises iteration}.


\newpage
\section{Singular value decomposition}
\label{app:svd}

\textit{SVD} decomposition is a particular factorization of a real or complex matrix. Formally, each arbitrary matrix $X \in \mathbb{R}^{n \times p}$ can be written as \cite{Jolliffe}:
\begin{equation}
    X=U\Sigma{V^T}
\end{equation}
where:
\begin{enumerate}
    \item $U$ and $V$ are $(n \times r)$, $(p \times r)$ matrices which have orthonormal columns. It means that $UU^T=I$ and $VV^T=I$, where $I$ is an identity matrix;
    \item $\Sigma$ is an $(r \times r)$ matrix;
    \item $r$ is a rank of $X$.
\end{enumerate}
If $X$ is a symmetric matrix, its decomposition is equivalent to:
\begin{equation}
    X=V\Sigma{V^T}
\end{equation}
The factorization with such properties can be computed using the following observations:
\begin{itemize}
    \item The columns of $U$ matrix are eigenvectors of $XX^T$.
    \item The columns of $V$ matrix are eigenvectors of $X^TX$.
    \item The values found on the diagonal entries of $\Sigma$ are square roots of non-zero eigenvalues of both $XX^T$ and $X^TX$.
\end{itemize}

Besides a few interesting mathematical properties, the \textit{SVD} decomposition has also an intuitive intepretation. If we assume that $X \in \mathbb{R}^{n \times n}$ is a square matrix of linear transformation with positive determinant, then $\Sigma$ can be thought as a scaling matrix while $V^T$ and $U$ can be viewed as rotation matrices. Therefore the expression $U\Sigma{V^T}$ can be regarded as a composition of three geometrical transformations: two rotations and scaling\cite{SVD}.

The standard \textit{SVD} algorithm has complexity of $O(np^2+p^3)$ and consists off two steps. In the first step, the origin matrix is transformed into bidiagonal matrix. It is usually made by \textit{Householder reflection} algorithm which is $O(np^2+p^3)$ in the worst case. The second step is connected with proceeding \textit{SVD} on the reduced matrix obtained in the first step. This part of the algorithm costs $O(p^2)$ and is computed by \textit{QR} decomposition \cite{SVD}. 

\newpage
\section{CCA computing algorithm based on \textit{SVD} decomposition}
\label{app:cca}

The algorithm based on \textit{SVD} is not the fastest way of computing \textit{CCA}. However, this approach is clear and shows the idea which is behind \textit{CCA}. Let $X$ and $Y$ be $(m \times n)$ and $(m \times k)$ matrices. The first step of the method is to find \textit{SVD} decomposition of $X$ and $Y$:
\begin{equation}
    X=U_1S_1V^T_1, \quad X=U_2S_2V^T_2
\end{equation}
Next, $U_1^TU_2$ matrix is formed and also decomposed by \textit{SVD}:
\begin{equation}
    U_1^TU_2 = Q\Sigma{L^T}
\end{equation}
Finally, matrices of coefficents of linear combinations for $X$ and $Y$ are computed \cite{William}:
\begin{equation}
    W_x=V_1S_1^{-1}Q, \quad  W_y=V_2S_2^{-1}L
\end{equation}


\chapter{Multi-label classification library in practise}
\section{Example of usage of \textit{Armadillo} library}
\label{app:arma}
\lstinputlisting[numbers=left, label=lst:arma1, caption=Armadillo example code, language=C++]{listings/1.cpp}

\newpage
\section{Build process management}
\label{app:build}

\newpage
\section{Example of learning flow}
\lstinputlisting[numbers=left, language=C++]{listings/2.cpp}

\newpage
\section{Example of evaluation flow}
\lstinputlisting[numbers=left, language=C++]{listings/3.cpp}

\end{appendices}
