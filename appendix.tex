\begin{appendices}

\chapter{Proofs and facts}

\section{Least squares for multiple regression}
\label{app:least}

Given a design matrix $\boldsymbol{X} \in \mathbb{R}^{m \times (n+1)}$ and a vector of real values $y \in \mathbb{R}^{n}$, we want to find $\boldsymbol{\beta}$, such that:
\begin{equation}
    \boldsymbol{\hat{\beta}} = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}}\sum\limits_{i=1}^{m}(\boldsymbol{y}_i-\sum\limits_{j=1}^{n+1}\boldsymbol{x}_{ij}\boldsymbol{\beta}_j)^2 = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^{n+1}}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})
\end{equation}
The above optimization problem can be simply transformed to the following equation:
\begin{equation}\label{eq:least1}
    \nabla_{\boldsymbol{\beta}} \frac{1}{2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}) = 0
\end{equation}
As we see, we need to calculate the derivative of $\frac{1}{2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})$ in respect to $\boldsymbol{\beta}$. In order to reach it we can use relations between derivatives with matrices and trace of a matrix. First of all, let us note a few facts about trace of a matrix:

\begin{enumerate}
    \item $tr(\boldsymbol{A}+\boldsymbol{B}) = tr(\boldsymbol{A})+tr(\boldsymbol{B})$
    \item $tr(\boldsymbol{AB}) = tr(\boldsymbol{BA})$
    \item $tr(\boldsymbol{ABC}) = tr(\boldsymbol{CAB}) = tr(\boldsymbol{BCA})$
    \item $\nabla_{\boldsymbol{A}}tr(\boldsymbol{AB}) = \boldsymbol{B}^T$
    \item $tr(\boldsymbol{A})=tr(\boldsymbol{A}^T)$
    \item if $a \in \mathbb{R}$ then $tr(a)=a$
    \item $\nabla_{\boldsymbol{A}}tr(\boldsymbol{ABA}^T\boldsymbol{C}) = \boldsymbol{CAB}+\boldsymbol{C}^T\boldsymbol{AB}^T$
\end{enumerate}
Calculating a dot product in \Cref{eq:least1} and using the property (6.) of a trace operator, we obtain:
\begin{equation}\label{eq:least2}
    \frac{1}{2} \nabla_{\boldsymbol{\beta}} tr(\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}-\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y}-\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{y}^T\boldsymbol{y}) = 0
\end{equation}
Next, using the properties (1.), (3.) and again (6.), \Cref{eq:least2} can be expressed as:
\begin{equation}\label{eq:least3}
    \frac{1}{2}[\nabla_{\boldsymbol{\beta}}tr(\boldsymbol{\beta}\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X})-\nabla_{\boldsymbol{\beta}}tr(\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta})-\nabla_{\boldsymbol{\beta}}tr(\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta})] = 0
\end{equation}
The derivative of $\boldsymbol{y}^T\boldsymbol{y}$ is $0$, because it does not depend on $\boldsymbol{\beta}$. Thus, it has been removed. Let us also notice that:
\begin{equation}\label{eq:least4}
    \nabla_{\boldsymbol{\beta}}tr(\boldsymbol{\beta}\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}) = \nabla_{\boldsymbol{\beta}}tr(\boldsymbol{\beta}\boldsymbol{I}\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}) = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}I + \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}\boldsymbol{I} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}
\end{equation}
because of the property (7.) of a trace operator and:
\begin{equation}\label{eq:least5}
    \nabla_{\boldsymbol{\beta}}tr(\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta}) = \boldsymbol{X}^T\boldsymbol{y}
\end{equation}
because of the property (4.).
Now we can simply use \Cref{eq:least4} and \Cref{eq:least5} in \Cref{eq:least3} and calculate the desired derivative:
\begin{equation}\label{eq:least6}
    \frac{1}{2}[\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}-\boldsymbol{X}^T\boldsymbol{y}-\boldsymbol{X}^T\boldsymbol{y}] = 0
\end{equation}
Finally, we obtain $\boldsymbol{\beta}$ from \cref{eq:least6}:
\begin{equation}
    \boldsymbol{\beta}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}

\newpage
\section{Eigenvalues and eigenvectors of matrices}
\label{app:eigen}

Let a matrix $\boldsymbol{T} \in \mathbb{R}^{m \times n}$ be a linear transformation matrix - such the matrix can express primary linear transformations, for instance rotation, scaling, transposition, etc. If there are a column vector $\boldsymbol{v}$ and a scalar $\lambda$, such that:
\begin{equation}\label{eq:eigen1}
    \boldsymbol{T}\boldsymbol{v}=\lambda{\boldsymbol{v}}
\end{equation}
then we call them an eigenvector and suitably an eigenvalue of a matrix $\boldsymbol{T}$. Intuitively an eigenvector $\boldsymbol{v}$ of a matrix $\boldsymbol{T}$ is transformed by $\boldsymbol{T}$ into itself - its direction does not change. An eigenvalue can be then interpreted as a scale factor. Formally eigenvectors and eigenvalues characterize an endomorphism of a particular linear space.  

The equation \ref{eq:eigen1} can be certainly stated equivalently as:

\begin{equation}\label{eq:eigen2}
    (\boldsymbol{T}-\lambda{\boldsymbol{I}})\boldsymbol{v}=0
\end{equation}
where $\boldsymbol{I}$ is the identity matrix. Let us notice that \cref{eq:eigen2} has a solution if and only if the determinant of a matrix $(\boldsymbol{T}-\lambda{\boldsymbol{I}})$ is equal to zero. As a result, we can find eigenvalues by solving the following equation:
\begin{equation}\label{eq:eigen3}
    det(\boldsymbol{T}-\lambda{\boldsymbol{I}}) = (\lambda_1-\lambda)(\lambda_2-\lambda)\cdots(\lambda_n-\lambda) = 0
\end{equation}
\Cref{eq:eigen3} is called the characteristic equation, while its left-hand side is called the characteristic polynomial. In practise this method is not used to calculate eigenvectors and eigenvalues. The most popular algorithm is \textit{Power iteration} which is also called \textit{Von Mises iteration}.


\newpage
\section{Singular value decomposition}
\label{app:svd}

SVD decomposition is a particular factorization of a real or a complex matrix. Formally, each arbitrary matrix $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ can be written as \cite{Jolliffe}:
\begin{equation}
    \boldsymbol{X}=\boldsymbol{U}\boldsymbol{\Sigma}{\boldsymbol{V}^T}
\end{equation}
where:
\begin{enumerate}
    \item $\boldsymbol{U}$ and $\boldsymbol{V}$ are $(n \times r)$, $(p \times r)$ matrices which have orthonormal columns. It means that $\boldsymbol{U}\boldsymbol{U}^T=\boldsymbol{I}$ and $\boldsymbol{V}\boldsymbol{V}^T=\boldsymbol{I}$, where $\boldsymbol{I}$ is the identity matrix;
    \item $\boldsymbol{\Sigma}$ is an $(r \times r)$ matrix;
    \item $r$ is a rank of $\boldsymbol{X}$.
\end{enumerate}
If $\boldsymbol{X}$ is a symmetric matrix, its decomposition is equivalent to:
\begin{equation}
    \boldsymbol{X}=\boldsymbol{V}\boldsymbol{\Sigma}{\boldsymbol{V}^T}
\end{equation}
The factorization with such properties can be computed using the following observations:
\begin{itemize}
    \item The columns of $\boldsymbol{U}$ matrix are eigenvectors of $\boldsymbol{X}\boldsymbol{X}^T$.
    \item The columns of $\boldsymbol{V}$ matrix are eigenvectors of $\boldsymbol{X}^T\boldsymbol{X}$.
    \item The values found on the diagonal entries of $\boldsymbol{\Sigma}$ are square roots of non-zero eigenvalues of both $\boldsymbol{X}\boldsymbol{X}^T$ and $\boldsymbol{X}^T\boldsymbol{X}$.
\end{itemize}

Besides a few interesting mathematical properties, the SVD decomposition has also an intuitive interpretation. If we assume that $\boldsymbol{X} \in \mathbb{R}^{n \times n}$ is a square matrix of a linear transformation with the positive determinant, then $\boldsymbol{\Sigma}$ can be thought as a scaling matrix while $\boldsymbol{V}^T$ and $\boldsymbol{U}$ can be viewed as rotation matrices. Therefore the expression $\boldsymbol{U}\boldsymbol{\Sigma}{\boldsymbol{V}^T}$ can be regarded as a composition of three geometrical transformations: two rotations and scaling\footnote{\bibentry{SVD}}.

The standard SVD algorithm has complexity of $O(np^2+p^3)$ and consists off two steps. In the first step, an origin matrix is transformed into a bidiagonal matrix. It is usually made by \textit{Householder reflection} algorithm which is $O(np^2+p^3)$ in the worst case. The second step is connected with proceeding SVD on a reduced matrix obtained in the first step. This part of the algorithm costs $O(p^2)$ and is computed by \textit{QR} decomposition\footnote{\bibentry{SVD}}. 

\newpage
\section{Invertibility of matrix in shrinkage regression}
\label{app:reg}

In this section we will show that it is not necessary to use pseudo inverse in shrinkage regression algorithm. Let us remind that the solution of shrinkage regression problem is given by the following formula:

\begin{equation}\label{eq:shrink}
    \boldsymbol{\hat{\beta}} = (\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}

In the proof we will use singular value decomposition. Using SVD of a symmetric matrix $\boldsymbol{X}^T\boldsymbol{X}$ and its properties, we can express $\boldsymbol{X}^T\boldsymbol{X}+\lambda\boldsymbol{I}$ as: 
\begin{equation}\label{eq:inv_proof}
\begin{split}
    \boldsymbol{X}^T\boldsymbol{X} + \lambda\boldsymbol{I}&=\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^T + \lambda\boldsymbol{I} = \boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^T + \lambda\boldsymbol{V}\boldsymbol{V}^T\boldsymbol{I}\boldsymbol{V}\boldsymbol{V}^T \\ 
    = &\boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{V}^T\boldsymbol{I}\boldsymbol{V})\boldsymbol{V}^T = \boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{I})\boldsymbol{V}^T 
\end{split}
\end{equation}
Let us notice that $\boldsymbol{V}(\boldsymbol{\Sigma} + \lambda\boldsymbol{I})\boldsymbol{V}^T$ is invertible, because $\boldsymbol{V}$ is invertible (it is orthogonal) and $\boldsymbol{\Sigma} +\lambda\boldsymbol{I}$ is invertible as well (this is a diagonal matrix and we have certainty that it does not have zeros on its diagonal).

\newpage
\section{CCA computing algorithm based on SVD decomposition}
\label{app:cca}

The algorithm based on SVD is not the fastest way of computing CCA. However, this approach is clear and shows the idea which is behind CCA. Let $\boldsymbol{X}$ and $\boldsymbol{Y}$ be $(m \times n)$ and $(m \times k)$ matrices. The first step of the method is to find SVD decomposition of $\boldsymbol{X}$ and $\boldsymbol{Y}$:
\begin{equation}
    \boldsymbol{X}=\boldsymbol{U}_1\boldsymbol{S}_1\boldsymbol{V}^T_1, \quad \boldsymbol{X}=\boldsymbol{U}_2\boldsymbol{S}_2\boldsymbol{V}^T_2
\end{equation}
Next, a matrix $\boldsymbol{U}_1^T\boldsymbol{U}_2$ is formed and also decomposed by SVD:
\begin{equation}
    \boldsymbol{U}_1^T\boldsymbol{U}_2 = \boldsymbol{Q}\boldsymbol{\Sigma}{\boldsymbol{L}^T}
\end{equation}
Finally, matrices of coefficients of linear combinations for $\boldsymbol{X}$ and $\boldsymbol{Y}$ are computed (\cite{William}):
\begin{equation}
    \boldsymbol{W}_x=\boldsymbol{V}_1\boldsymbol{S}_1^{-1}\boldsymbol{Q}, \quad  \boldsymbol{W}_y=\boldsymbol{V}_2\boldsymbol{S}_2^{-1}\boldsymbol{L}
\end{equation}


\chapter{Multi-label classification library in practise}
\section{Example of usage of Armadillo library}
\label{app:arma}
\lstinputlisting[numbers=left, language=C++, caption=armadillo.cpp]{listings/armadillo.cpp}

\newpage
\section{Example of data in ARFF format}
\label{app:arff}
\lstinputlisting[caption=iris.arff]{listings/iris.arff}
\lstinputlisting[caption=iris.xml, language=xml]{listings/iris.xml}

\newpage
\label{app:learning}
\section{Example of learning flow}
\lstinputlisting[numbers=left, language=C++, caption=learning.cpp]{listings/learning.cpp}

\newpage
\label{app:evaluating}
\section{Example of evaluation flow}
\lstinputlisting[numbers=left, language=C++, caption=evaluation.cpp]{listings/evaluation.cpp}

\end{appendices}
